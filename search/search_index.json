{
    "docs": [
        {
            "location": "/",
            "text": "LTRDCN-1572\n\n\nWelcome to Cisco Live LTRDCN-1572: VXLAN EVPN Fabric and automation using Ansible.\n\n\nFor full documentation visit \nCisco Live\n.\n\n\nSpeakers\n\n\n\n\nFaisal Chaudhry \nPrincipal Architect, Cisco Advanced Services\n\n\nLei Tian \nSolutions Architect, Cisco Advanced Services",
            "title": "Home"
        },
        {
            "location": "/#ltrdcn-1572",
            "text": "Welcome to Cisco Live LTRDCN-1572: VXLAN EVPN Fabric and automation using Ansible.  For full documentation visit  Cisco Live .",
            "title": "LTRDCN-1572"
        },
        {
            "location": "/#speakers",
            "text": "Faisal Chaudhry  Principal Architect, Cisco Advanced Services  Lei Tian  Solutions Architect, Cisco Advanced Services",
            "title": "Speakers"
        },
        {
            "location": "/intro/",
            "text": "VXLAN\n\n\nVXLAN stands for Virtual Extensible Local Area Network. VXLAN is a L2 overlay scheme on top of L3 network or we can say it is a L2 in layer 3 tunnel. It runs over the existing networks and provides the means to stretch the L2 network. Only VMs within the same VXLAN segment can communicate with each other. Each VXLAN segment is identified by a 24 bit segment ID called \u201cVXLAN Network Identifier (VNI)\u201d.  This help overcome 4094 VLAN scale limitation and able to extend it to 224 segments.\n\n\nVXLAN uses BGP as its control plane for Overlay. It makes it forwarding decisions at VTEPs (Virtual tunnel end points) for layer-2 and layer-3. Forwarding happens based on MAC or IP learnt via control plane (MP-BGP EVPN) . VXLAN uses IGP, PIM and BGP as its underlay in the fabric. \n\n\nBelow are some of the terminologies that will be used in the lab:\n\n\n\n\nVNI / VNID\n \u2013 VXLAN Network Identifier. This replaces VLAN ID \n\n\nVTEP\n \u2013 VXLAN Tunnel End Point.\n\n\nThis is the end point where the box performs VXLAN encap / decap\nThis could be physical HW (Nexus9k) or Virtual (Nexus 1000v, Nexus 9000v)\n\n\n\n\n\n\nVXLAN Segment\n -  The resulting layer 2 overlay network\n\n\nVXLAN Gateway\n \u2013 It is a device that forwards traffic between VXLANS. It can be both L2 and L3 forwarding\n\n\nNVE\n \u2013 Network Virtualization Edge\n\n\nNVE is tunnel interface. It represents VTEP\n\n\n\n\n\n\n\n\nAnsible\n\n\nAnsible is an agentless open source software that can be used for configuration management, deployment and orchestration of deployment. The scripts in Ansible are called playbooks; playbook is in YAML format that was desgiened to be easy for humans to read and write. Playbooks include one or more plays, each play include one or more tasks. Each task is associated with one module, which is what gets executed in the playbook. Modules are python scripts that ship with Ansible installation. During the lab, you will be introduced to multiple NXOS modules and ansible template module. \n\n\nYou can find all Ansible modules documentation at below url:\n\nhttp://docs.ansible.com/ansible/latest/list_of_all_modules.html\n\n\nBelow are some of the terminologies that will be used in the lab:\n\n\n\n\nHost\n: remote machines that Ansible manages  \n\n\nGroup\n: several hosts that can be configured together and share common verables \n\n\nInventory\n: file descripts hosts and groups in Ansible.\n\n\nVariable\n: names of value (int, str, dic, list) referenced in playbook or template\n\n\nYAML\n: data format for Playbook or Variables in Ansible \n\n\nPlaybook\n: the script to orchestrate, automate, deploy system in Ansible. One playbook can include multiple plays. \n\n\nRoles\n: group of tasks, templates to implement specific behavior\n\n\nJinja2\n: a Python based tempting language\n\n\n\n\n\n\nAbout this lab\n\n\nAs a standardized overlay technology, multiple vendors have adopted VXLAN as datacenter solution to provide scalability and allow layer 2 across IP network. MP-BPG EVPN as VXLAN control plane protocol provides a robust scalable solution to overcome the limitation in VXLAN flood and learn mode.\n\n\nAs an open source automation tool, Ansible provides the same framework for network administrators to automate network infrastructure as the rest IT organization. \n\n\nThis lab demostates the possibility of using Ansible to automate datacenter VXLAN fabric day 1 provisiong and day 2 operations. \n\n\nLab Flow\n\n\nLab guide will walk the attendees through the below activities:\n\n\n\n\nAnsible installation \n\n\nAnsible playbook \n\n\nDay 1 automation using Ansible \n\n\nDay 2 automation using Ansible \n\n\nDay 0 automation \n\n\nL4-L7 Service insertion\n\n\n\n\nLab Access\n\n\nBelow table provides the IP addresses and credentials for the devices used in this lab: \n\n\n\n\n\n\n\n\nSpine-1\n\n\n198.18.133.33:1030\n\n\nadmin/C1sco12345\n\n\n\n\n\n\n\n\n\n\nSpine-2\n\n\n198.18.133.33:1040\n\n\nadmin/C1sco12345\n\n\n\n\n\n\nLeaf-1\n\n\n198.18.133.33:1050\n\n\nadmin/C1sco12345\n\n\n\n\n\n\nLeaf-3\n\n\n198.18.133.33:1070\n\n\nadmin/C1sco12345\n\n\n\n\n\n\nLeaf-4\n\n\n198.18.1333.33:1080\n\n\nadmin/C1sco12345\n\n\n\n\n\n\nServer-1\n\n\n198.18.134.50\n\n\nroot/C1sco12345\n\n\n\n\n\n\nServer-3\n\n\n198.18.134.52\n\n\nroot/C1sco12345\n\n\n\n\n\n\nServer-4\n\n\n198.18.134.53\n\n\nroot/C1sco12345\n\n\n\n\n\n\nAnsible Server\n\n\n198.18.134.150\n\n\nroot/C1sco12345\n\n\n\n\n\n\nDCNM\n\n\n198.18.134.200\n\n\nadmin/C1sco12345\n\n\n\n\n\n\nF5\n\n\n198.18.4.10\n\n\nroot/default\n\n\n\n\n\n\nRemote Workstation\n\n\n198.18.133.36\n\n\ndemouser/C1sco12345\n\n\n\n\n\n\n\n\nLab topology\n\n\nBelow picture shows the lab topology:",
            "title": "Introduction"
        },
        {
            "location": "/intro/#vxlan",
            "text": "VXLAN stands for Virtual Extensible Local Area Network. VXLAN is a L2 overlay scheme on top of L3 network or we can say it is a L2 in layer 3 tunnel. It runs over the existing networks and provides the means to stretch the L2 network. Only VMs within the same VXLAN segment can communicate with each other. Each VXLAN segment is identified by a 24 bit segment ID called \u201cVXLAN Network Identifier (VNI)\u201d.  This help overcome 4094 VLAN scale limitation and able to extend it to 224 segments.  VXLAN uses BGP as its control plane for Overlay. It makes it forwarding decisions at VTEPs (Virtual tunnel end points) for layer-2 and layer-3. Forwarding happens based on MAC or IP learnt via control plane (MP-BGP EVPN) . VXLAN uses IGP, PIM and BGP as its underlay in the fabric.   Below are some of the terminologies that will be used in the lab:   VNI / VNID  \u2013 VXLAN Network Identifier. This replaces VLAN ID   VTEP  \u2013 VXLAN Tunnel End Point.  This is the end point where the box performs VXLAN encap / decap\nThis could be physical HW (Nexus9k) or Virtual (Nexus 1000v, Nexus 9000v)    VXLAN Segment  -  The resulting layer 2 overlay network  VXLAN Gateway  \u2013 It is a device that forwards traffic between VXLANS. It can be both L2 and L3 forwarding  NVE  \u2013 Network Virtualization Edge  NVE is tunnel interface. It represents VTEP",
            "title": "VXLAN"
        },
        {
            "location": "/intro/#ansible",
            "text": "Ansible is an agentless open source software that can be used for configuration management, deployment and orchestration of deployment. The scripts in Ansible are called playbooks; playbook is in YAML format that was desgiened to be easy for humans to read and write. Playbooks include one or more plays, each play include one or more tasks. Each task is associated with one module, which is what gets executed in the playbook. Modules are python scripts that ship with Ansible installation. During the lab, you will be introduced to multiple NXOS modules and ansible template module.   You can find all Ansible modules documentation at below url: http://docs.ansible.com/ansible/latest/list_of_all_modules.html  Below are some of the terminologies that will be used in the lab:   Host : remote machines that Ansible manages    Group : several hosts that can be configured together and share common verables   Inventory : file descripts hosts and groups in Ansible.  Variable : names of value (int, str, dic, list) referenced in playbook or template  YAML : data format for Playbook or Variables in Ansible   Playbook : the script to orchestrate, automate, deploy system in Ansible. One playbook can include multiple plays.   Roles : group of tasks, templates to implement specific behavior  Jinja2 : a Python based tempting language",
            "title": "Ansible"
        },
        {
            "location": "/intro/#about-this-lab",
            "text": "As a standardized overlay technology, multiple vendors have adopted VXLAN as datacenter solution to provide scalability and allow layer 2 across IP network. MP-BPG EVPN as VXLAN control plane protocol provides a robust scalable solution to overcome the limitation in VXLAN flood and learn mode.  As an open source automation tool, Ansible provides the same framework for network administrators to automate network infrastructure as the rest IT organization.   This lab demostates the possibility of using Ansible to automate datacenter VXLAN fabric day 1 provisiong and day 2 operations.",
            "title": "About this lab"
        },
        {
            "location": "/intro/#lab-flow",
            "text": "Lab guide will walk the attendees through the below activities:   Ansible installation   Ansible playbook   Day 1 automation using Ansible   Day 2 automation using Ansible   Day 0 automation   L4-L7 Service insertion",
            "title": "Lab Flow"
        },
        {
            "location": "/intro/#lab-access",
            "text": "Below table provides the IP addresses and credentials for the devices used in this lab:      Spine-1  198.18.133.33:1030  admin/C1sco12345      Spine-2  198.18.133.33:1040  admin/C1sco12345    Leaf-1  198.18.133.33:1050  admin/C1sco12345    Leaf-3  198.18.133.33:1070  admin/C1sco12345    Leaf-4  198.18.1333.33:1080  admin/C1sco12345    Server-1  198.18.134.50  root/C1sco12345    Server-3  198.18.134.52  root/C1sco12345    Server-4  198.18.134.53  root/C1sco12345    Ansible Server  198.18.134.150  root/C1sco12345    DCNM  198.18.134.200  admin/C1sco12345    F5  198.18.4.10  root/default    Remote Workstation  198.18.133.36  demouser/C1sco12345",
            "title": "Lab Access"
        },
        {
            "location": "/intro/#lab-topology",
            "text": "Below picture shows the lab topology:",
            "title": "Lab topology"
        },
        {
            "location": "/task1-ansible-node/",
            "text": "Your first task will be to build an Ansible node on a server running redhat CentOS operating system.  At the end of this task, you will have a fully operational Ansible node.\n\n\nStep 1: Connect to lab using anyconnect VPN\n\n\nYou will connect to \ndcloud-lon-anyconnect.cisco.com\n using Cisco VPN AnyConnect client, as shown in below picture, with the username and password provided by the lab admin.\n\n\nNote:\n lab admin will furnish the credentials information to the participant.  If you don't have this information please ask the lab speakers.\n\n\n\n\nStep 2: Enter VPN credentials\n\n\nAfter prompted for credentials, use the credentials provided by the lab admin.    \n\n\u2022   Below is an example of user logging into POD1\n\n\n\n\n\n\nHit accept when the prompt appears to accept the VPN connection login    \n\n\n\n\n\n\nStep 3: RDP to workstation\n\n\nIn this step, you will connect to the workstation with RDP client on your machines.  Use below details for this RDP session:\n\n\n\n\nWorkstation: \n198.18.133.36\n\n\nUsername: \ndcloud\\demouser\n\n\nPassword: \nC1sco12345\n\n\n\n\nBelow screenshot is only an example for this RDP connection:\n\n\n\n\nStep 4: MTputty\n\n\nOnce you have the RDP session to the remote workstation, then you will use MTputty client to connect to all devices in this lab.  \n\n\nMTputty is already installed on the Desktop of the workstation where you connected using RDP.  Run this application by clicking on the icon on the desktop:\n\n\n\n\nStep 5: SSH into Ansible node\n\n\nSSH into Ansible node (198.18.134.150) by double clicking the Ansible icon on the left pan with username \nroot\n and password \nC1sco12345\n\n\n\n\nStep 6: Verify Python\n\n\nOnce successfully SSH into the ansible node, the very first thing we are going to do after logging into Ansible server is verify the python version by running \npython --version\n command - as shown below:\n\n\n[root@rhel7-tools ~]# python --version\nPython 2.7.5\n\n\n\nIt is an important step as we need minimum 2.7.5 version of python in order to install some features for ansbile.  The output of above command confirms this version. \n\n\nAnsible can be run from any machine with Python 2 (versions 2.6 or 2.7) or Python 3 (versions 3.5 and higher) installed. \n\n\n\n\nStep 7: Install PIP\n\n\nAfter verifying we have the minimum version of python installed, we are now going to Install PIP python package using \neasy_install pip\n command as shown below:\n\n\n[root@rhel7-tools ~]# easy_install pip\nSearching for pip\nBest match: pip 8.1.1\nAdding pip 8.1.1 to easy-install.pth file\nInstalling pip script to /usr/bin\nInstalling pip3.5 script to /usr/bin\nInstalling pip3 script to /usr/bin\n\nUsing /usr/lib/python2.7/site-packages\nProcessing dependencies for pip\nFinished processing dependencies for pip\n\n\n\nBelow screenshot shows the execution of above command:\n\n\n\n\nNext, update pip to latest version by executing below command:\n\n\n[root@rhel7-tools ~]# pip install --upgrade pip\n\n\n\nBelow screenshot shows the exection of above command:\n\n\n\n\nAfter installing PIP package, we are going to add the relevant packages that are needed for this Ansible based VXLAN lab. Below are the packages required for this lab.  \n\n\n\n\nParamiko\n\n\nPyYAML\n\n\nJinj2\n\n\nHttplib2\n\n\n\n\nRun below command to install these packages:\n\n\n[root@rhel7-tools ~]# pip install paramiko PyYAML jinja2 httplib2\n\n\n\nBelow screenshot shows the execution of above command:\n\n\n\n\nAs a final step, we are going to install Ansible on this RHEL. Once the install is initiated with the below command, it will take few minutes for it to download and install.\n\n\n[root@rhel7-tools ~]# pip install ansible==2.5.0\n\n\n\nBelow screenshot shows the execution of above command:\n\n\n\n\nStep 8: Verify Ansible\n\n\nAfter installation is complete, check Ansible version by executing command \nansible --version\n, as shown below:\n\n\n[root@rhel7-tools ~]# ansible --version \nansible 2.5.0\n  config file = None\n  configured module search path = [u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']\n  ansible python module location = /usr/lib/python2.7/site-packages/ansible\n  executable location = /usr/bin/ansible\n  python version = 2.7.5 (default, Apr  9 2015, 11:03:32) [GCC 4.8.3 20140911 (Red Hat 4.8.3-9)]\n[root@rhel7-tools ~]#\n\n\n\nStep 9: Create Ansible Inventory\n\n\nNow, we are going to create inventory, host variables and Configuration file. This is important as Ansible works  against multiple systems in the system by selecting portions of systems listed in Ansible inventory. Similarly, configuration settings in Ansible are adjustable via configuration file.\n\n\nCreate folder named LTRDCN-1572 as working environment and verify that it\u2019s empty:\n\n\n[root@rhel7-tools ~]# mkdir LTRDCN-1572 && cd LTRDCN-1572\n[root@rhel7-tools LTRDCN-1572]# ls\n\n\n\nBelow screenshot shows the execution of above command:\n\n\n\n\nNext:\n\n\n\n\nCreate Ansible inventory file to include Spine and Leaf switches. \n\n\nBy default Ansible has inventory file saved in location /etc/ansible/hosts. \n\n\n\n\nIn this lab we will create hosts file in the working environment. Use \u2018vi\u2019 to create inventory file \u2018hosts\u2019 as shown below:\n\n\n[root@rhel7-tools LTRDCN-1572]# vi hosts\n\n\n\n\n\n\n\nAt the bottom of the inventory file, insert (type i) the following lines:\n\n\n#define global variables, groups and host variables\n[all:vars]\nansible_connection = local\nuser=admin\npwd=C1sco12345\ngather_fact=no\n[jinja2_spine]\n198.18.4.202\n[jinja2_leaf]\n198.18.4.104\n[spine]\n198.18.4.201\n[leaf]\n198.18.4.101\n198.18.4.103\n[server]\n198.18.134.50 eth1=172.21.140.10 gw=172.21.140.1\n198.18.134.52 eth1=172.21.140.11 gw=172.21.140.1\n198.18.134.53 eth1=172.21.141.11 gw=172.21.141.1\n[f5]\n198.18.4.10\n\n\n\n\n\n\n\nQuit and save vi editor by press \nEsc\n key and then type \n:wq!\n  \n\n\nBelow screenshot shows the execution of above command:\n\n\n\n\n\n\n\nCreate Ansible config (ansible.cfg) file via vi editor pointing to local \u2018hosts\u2019 file for inventory. Use \nvi ansible.cfg\n to create this file as shown below:\n\n\n[root@rhel7-tools LTRDCN-1572]# vi ansible.cfg\n\n\n\n\n\n\n\nAt the bottom of the ansible.cfg file, insert the below lines (by typing \ni\n):\n\n\n[defaults]\ninventory = hosts\nhost_key_checking = false\nrecord_host_key = true\nstdout_callback = debug\n\n\n\n\n\n\n\nQuit and save \nvi\n editor by press \nEsc\n key and then type \n:wq!\n    \n\n\n\n\n\n\nDo \nls\n to verify the file that you just created under project folder LTRDCN-1572.   \n\n\nBelow screenshot shows the execution of above command:\n\n\n\n\n\n\n\nCreate host variable folder named host_vars in folder LTRDCN-1572. Host varilables can be placed in different places in Ansible. In this lab, we will use host_vars for host variables:\n\n\n[root@rhel7-tools LTRDCN-1572]# mkdir host_vars && cd host_vars\n\n\n\n\n\n\n\nCreate host variable file for each host in inventory. Use \nvi\n to create following file: \n\n\n[root@rhel7-tools host_vars]# vi 198.18.4.101.yml\n\n\n\nIn the file: add below data:\n\n\n---\n  hostname: leaf_1\n  loopback0: 192.168.0.8\n  loopback1: 192.168.0.18\n  router_id: 192.168.0.8\n\n\n\n\n\n\n\nQuit and save \nvi\n editor by pressing \nEsc\n key and then type \n:wq!\n     \n\n\n\n\n\n\nCreate a new host variable file for next host in inventory. Use \nvi\n to create following file: \n\n\n[root@rhel7-tools host_vars]# vi 198.18.4.201.yml\n\n\n\nIn the file: add below data:\n\n\n---\n  hostname: spine-1\n  loopback0: 192.168.0.6\n  loopback1: 192.168.0.100\n  router_id: 192.168.0.6\n\n\n\n\n\n\n\nQuit and save \nvi\n editor by press \nEsc\n key and then type \n:wq!\n \n\n\n\n\n\n\nCreate a new host variable file for next host in inventory. Use \nvi\n to create following file: \n\n\n[root@rhel7-tools host_vars]# vi 198.18.4.202.yml\n\n\n\nIn the file: add below data:\n\n\n---\n  hostname: spine-2\n  loopback0: 192.168.0.7\n  loopback1: 192.168.0.100\n  router_id: 192.168.0.7\n\n\n\n\n\n\n\nQuit and save \nvi\n editor by press \nEsc\n key and then type \n:wq!\n \n\n\n\n\n\n\nStep 10: Ansible role structure\n\n\nRole is very useful technique to manage a set of playbooks in Ansible. \nIn this lab, we will use two different playbooks to manage configuration for Spine and Leaf switches. \n\n\nWe will use role structure and manage the two plays into single playbook. \nA role directory structure contains several directories of defaults, vars, files, handlers, meta, tasks and templates. \n\n\nIn this lab:\n\n\n\n\nwe will use vars, templates and tasks folders \n/vars\n \n\n\nmain.yml file in vars folder contains dictionary of variables for this role \n/tasks\n\n\nmain.yml file in tasks folder contains the Ansible playbook for this role\n\n\n\n\nTo proceed further in the new:\n\n\n\u2022   Create roles directory in folder LTRDCN-1572 by issuing below commands:\n\n\n[root@rhel7-tools LTRDCN-1572]# mkdir roles\n\n\n\nThis will be used in the lab later.",
            "title": "Task 1 - Prepare Ansible node"
        },
        {
            "location": "/task1-ansible-node/#step-1-connect-to-lab-using-anyconnect-vpn",
            "text": "You will connect to  dcloud-lon-anyconnect.cisco.com  using Cisco VPN AnyConnect client, as shown in below picture, with the username and password provided by the lab admin.  Note:  lab admin will furnish the credentials information to the participant.  If you don't have this information please ask the lab speakers.",
            "title": "Step 1: Connect to lab using anyconnect VPN"
        },
        {
            "location": "/task1-ansible-node/#step-2-enter-vpn-credentials",
            "text": "After prompted for credentials, use the credentials provided by the lab admin.     \n\u2022   Below is an example of user logging into POD1    Hit accept when the prompt appears to accept the VPN connection login",
            "title": "Step 2: Enter VPN credentials"
        },
        {
            "location": "/task1-ansible-node/#step-3-rdp-to-workstation",
            "text": "In this step, you will connect to the workstation with RDP client on your machines.  Use below details for this RDP session:   Workstation:  198.18.133.36  Username:  dcloud\\demouser  Password:  C1sco12345   Below screenshot is only an example for this RDP connection:",
            "title": "Step 3: RDP to workstation"
        },
        {
            "location": "/task1-ansible-node/#step-4-mtputty",
            "text": "Once you have the RDP session to the remote workstation, then you will use MTputty client to connect to all devices in this lab.    MTputty is already installed on the Desktop of the workstation where you connected using RDP.  Run this application by clicking on the icon on the desktop:",
            "title": "Step 4: MTputty"
        },
        {
            "location": "/task1-ansible-node/#step-5-ssh-into-ansible-node",
            "text": "SSH into Ansible node (198.18.134.150) by double clicking the Ansible icon on the left pan with username  root  and password  C1sco12345",
            "title": "Step 5: SSH into Ansible node"
        },
        {
            "location": "/task1-ansible-node/#step-6-verify-python",
            "text": "Once successfully SSH into the ansible node, the very first thing we are going to do after logging into Ansible server is verify the python version by running  python --version  command - as shown below:  [root@rhel7-tools ~]# python --version\nPython 2.7.5  It is an important step as we need minimum 2.7.5 version of python in order to install some features for ansbile.  The output of above command confirms this version.   Ansible can be run from any machine with Python 2 (versions 2.6 or 2.7) or Python 3 (versions 3.5 and higher) installed.",
            "title": "Step 6: Verify Python"
        },
        {
            "location": "/task1-ansible-node/#step-7-install-pip",
            "text": "After verifying we have the minimum version of python installed, we are now going to Install PIP python package using  easy_install pip  command as shown below:  [root@rhel7-tools ~]# easy_install pip\nSearching for pip\nBest match: pip 8.1.1\nAdding pip 8.1.1 to easy-install.pth file\nInstalling pip script to /usr/bin\nInstalling pip3.5 script to /usr/bin\nInstalling pip3 script to /usr/bin\n\nUsing /usr/lib/python2.7/site-packages\nProcessing dependencies for pip\nFinished processing dependencies for pip  Below screenshot shows the execution of above command:   Next, update pip to latest version by executing below command:  [root@rhel7-tools ~]# pip install --upgrade pip  Below screenshot shows the exection of above command:   After installing PIP package, we are going to add the relevant packages that are needed for this Ansible based VXLAN lab. Below are the packages required for this lab.     Paramiko  PyYAML  Jinj2  Httplib2   Run below command to install these packages:  [root@rhel7-tools ~]# pip install paramiko PyYAML jinja2 httplib2  Below screenshot shows the execution of above command:   As a final step, we are going to install Ansible on this RHEL. Once the install is initiated with the below command, it will take few minutes for it to download and install.  [root@rhel7-tools ~]# pip install ansible==2.5.0  Below screenshot shows the execution of above command:",
            "title": "Step 7: Install PIP"
        },
        {
            "location": "/task1-ansible-node/#step-8-verify-ansible",
            "text": "After installation is complete, check Ansible version by executing command  ansible --version , as shown below:  [root@rhel7-tools ~]# ansible --version \nansible 2.5.0\n  config file = None\n  configured module search path = [u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']\n  ansible python module location = /usr/lib/python2.7/site-packages/ansible\n  executable location = /usr/bin/ansible\n  python version = 2.7.5 (default, Apr  9 2015, 11:03:32) [GCC 4.8.3 20140911 (Red Hat 4.8.3-9)]\n[root@rhel7-tools ~]#",
            "title": "Step 8: Verify Ansible"
        },
        {
            "location": "/task1-ansible-node/#step-9-create-ansible-inventory",
            "text": "Now, we are going to create inventory, host variables and Configuration file. This is important as Ansible works  against multiple systems in the system by selecting portions of systems listed in Ansible inventory. Similarly, configuration settings in Ansible are adjustable via configuration file.  Create folder named LTRDCN-1572 as working environment and verify that it\u2019s empty:  [root@rhel7-tools ~]# mkdir LTRDCN-1572 && cd LTRDCN-1572\n[root@rhel7-tools LTRDCN-1572]# ls  Below screenshot shows the execution of above command:   Next:   Create Ansible inventory file to include Spine and Leaf switches.   By default Ansible has inventory file saved in location /etc/ansible/hosts.    In this lab we will create hosts file in the working environment. Use \u2018vi\u2019 to create inventory file \u2018hosts\u2019 as shown below:  [root@rhel7-tools LTRDCN-1572]# vi hosts    At the bottom of the inventory file, insert (type i) the following lines:  #define global variables, groups and host variables\n[all:vars]\nansible_connection = local\nuser=admin\npwd=C1sco12345\ngather_fact=no\n[jinja2_spine]\n198.18.4.202\n[jinja2_leaf]\n198.18.4.104\n[spine]\n198.18.4.201\n[leaf]\n198.18.4.101\n198.18.4.103\n[server]\n198.18.134.50 eth1=172.21.140.10 gw=172.21.140.1\n198.18.134.52 eth1=172.21.140.11 gw=172.21.140.1\n198.18.134.53 eth1=172.21.141.11 gw=172.21.141.1\n[f5]\n198.18.4.10    Quit and save vi editor by press  Esc  key and then type  :wq!     Below screenshot shows the execution of above command:    Create Ansible config (ansible.cfg) file via vi editor pointing to local \u2018hosts\u2019 file for inventory. Use  vi ansible.cfg  to create this file as shown below:  [root@rhel7-tools LTRDCN-1572]# vi ansible.cfg    At the bottom of the ansible.cfg file, insert the below lines (by typing  i ):  [defaults]\ninventory = hosts\nhost_key_checking = false\nrecord_host_key = true\nstdout_callback = debug    Quit and save  vi  editor by press  Esc  key and then type  :wq!         Do  ls  to verify the file that you just created under project folder LTRDCN-1572.     Below screenshot shows the execution of above command:    Create host variable folder named host_vars in folder LTRDCN-1572. Host varilables can be placed in different places in Ansible. In this lab, we will use host_vars for host variables:  [root@rhel7-tools LTRDCN-1572]# mkdir host_vars && cd host_vars    Create host variable file for each host in inventory. Use  vi  to create following file:   [root@rhel7-tools host_vars]# vi 198.18.4.101.yml  In the file: add below data:  ---\n  hostname: leaf_1\n  loopback0: 192.168.0.8\n  loopback1: 192.168.0.18\n  router_id: 192.168.0.8    Quit and save  vi  editor by pressing  Esc  key and then type  :wq!          Create a new host variable file for next host in inventory. Use  vi  to create following file:   [root@rhel7-tools host_vars]# vi 198.18.4.201.yml  In the file: add below data:  ---\n  hostname: spine-1\n  loopback0: 192.168.0.6\n  loopback1: 192.168.0.100\n  router_id: 192.168.0.6    Quit and save  vi  editor by press  Esc  key and then type  :wq!      Create a new host variable file for next host in inventory. Use  vi  to create following file:   [root@rhel7-tools host_vars]# vi 198.18.4.202.yml  In the file: add below data:  ---\n  hostname: spine-2\n  loopback0: 192.168.0.7\n  loopback1: 192.168.0.100\n  router_id: 192.168.0.7    Quit and save  vi  editor by press  Esc  key and then type  :wq!",
            "title": "Step 9: Create Ansible Inventory"
        },
        {
            "location": "/task1-ansible-node/#step-10-ansible-role-structure",
            "text": "Role is very useful technique to manage a set of playbooks in Ansible. \nIn this lab, we will use two different playbooks to manage configuration for Spine and Leaf switches.   We will use role structure and manage the two plays into single playbook. \nA role directory structure contains several directories of defaults, vars, files, handlers, meta, tasks and templates.   In this lab:   we will use vars, templates and tasks folders  /vars    main.yml file in vars folder contains dictionary of variables for this role  /tasks  main.yml file in tasks folder contains the Ansible playbook for this role   To proceed further in the new:  \u2022   Create roles directory in folder LTRDCN-1572 by issuing below commands:  [root@rhel7-tools LTRDCN-1572]# mkdir roles  This will be used in the lab later.",
            "title": "Step 10: Ansible role structure"
        },
        {
            "location": "/task2-first-ansible/",
            "text": "Task 2: First Simple Ansible Playbook\n\n\nIn this section, we will create the first Ansible Playbook. In the playbook, we will configure VLANs on leaf switches, and assign VLANs to the server facing port. \n\n\nYou will learn create variables inside playbook, learn simple loop using \u201c\nwith_items\n\u201d, learn simple logical using \u201c\nwhen\n\u201d and learn \u201c\ntags\n\u201d to isolate tasks from whole playbook. \n\n\n\n\nStep 1: Using \"Atom\" text Editor\n\n\n\n\n\n\nOpen \u201cAtom\u201d text editor by double click the icon on desktop. Atom is the recommended text editor for this lab:\n\n\n\n\n\n\n\n\nAfter opening ATOM, Click \u201cNo Never\u201d to the \u201cRegister as default atom:// URI handlre\u201d message as show below:\n\n\n\n\n\n\n\n\n\n\nStep 2: Atom folder\n\n\n\n\nAfter opening ATOM, there should be a folder in the left pane named \u201cLTRDCN-1572.\u201d\n\n\n\n\nRight click the pre-configured project folder \nLTRDCN-1572\n and select \nNew File\n \n\n\n\n\n\n\n\n\nName the new file \nvlan_provision.yml\n and hit enter. This will creat the new file:\n\n\n\n\n\n\n\n\n\n\nStep 3: Define variables, tasks for playbook\n\n\nIn this step, we are going to define the scope, variable for playbook and tasks \n\n\n\n\nIn the opened window for \u2018vlan_provision.yml\u2019 file, enter the below content.\n\n\nNOTE: YAML is space sensitive. Hence be careful with the spaces in below section. \n\n\n\n\n\n\n\n\n---                     \n#Task 2: Simple playbook assign VLAN to server facing port\n  - hosts: leaf:jinja2_leaf\n    vars:\n      nxos_provider:\n        username: \"{{ user }}\"\n        password: \"{{ pwd }}\"\n        transport: nxapi\n        host: \"{{ inventory_hostname }}\"\n\n\n\n\n\n\n\u201c\nhosts\n\u201d defines the scope of this playbook applies to all switches in group \u2018leaf\u2019 and \u2018jinja2_leaf\u2019. \n\n\nNote\n you can review the IP addresses of the three (2) \u201cleaf\u201d and one (1) \u201cjinja2_leaf\u201d in the \u201c\nhosts\n\u201d file (configured in previous steps).  The IP addresses are:\n\n\njinja2_leaf: 198.18.4.104\n\n\nleaf: 198.18.4.101\n\n\nleaf: 198.18.4.103\n\n\n\n\n\n\n\n\n\n\n\u201c\nvars\n\u201d defines one variable \u201c\nnxos_provider\n\u201d that will be used in this playbook. \n\n\n\u201c\nnxos_provider\n\u201d is variable that includes all value that will be used for connection and autnentication. \n\n\nThis variable will be referenced in playbook via \u201c\nprovider\n\u201d argument.     \n\n\n\n\n\n\n\n\n\n\nStep 4: VLAN tasks in playbook\n\n\n\n\nContinue to add below tasks on the playbook:\n\n\n\n\n    tasks:\n      - name: provision VLAN\n        nxos_config:\n          lines: \"vlan {{item}}\"\n          provider: \"{{nxos_provider}}\"\n        with_items:\n          -  140\n          -  141\n        tags: add vlans\n\n\n\n\n\n\nMultiple plays can be defined in one playbook under \u201c\ntasks\n\u201d, each starts with \u201c-\u201c . \n\n\n\n\nThis kind of play creates multiple VLANs using \nnxos_config\n module. \n\n\n\n\nThe \u201clines\u201d option is used to pass only one configuration command.  This commands must be the exact same commands as found in the device running-config.\n\n\nThough one or multiple configuration commands can be configured under the \u201clines\u201d option.   For multiple, ordered set of commands can be configured under this \u201cline\u201d section.\n\n\n\n\n\n\n\n\nAt the end of this play, we use \u201c\ntags\n\u201d to name the play \u201c\nadd vlans\n\u201d. This is useful to run a specific part of the configuration without running whole playbook. \n\n\n\n\nNOTE: Formatting is extremely important when working with Ansible. Ansible playbook would return errors if the spaces are not properly aligned or formatting is not correct\n\n\n\n\n\n\n\nClick \u201c\nFile\n\u201d and \u201c\nSave\n\u201d . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\nNOTE: Once the Save button is pressed, then at the lower part of ATOM app, you will see message about connecting to Ansibe host (198.18.134.150) and saving the vlan_provision.yml file\n\n\n\n\n\n\n\n\nStep 5: Execute playbook\n\n\nAfter creating the playbook, it is now time to execute the playbook. \n\n\n\n\nBefore executing the playbook, we will verify the leaf switch if it has any vlan configuration present on it.\n\n\n\n\nLogin\n to leaf-3 switch using the Mputty client, or any leaf switch and execute \nshow vlan brief\n command. \n\n\n\n\nThis command will show the vlans that currently exist on the leaf switch.  As you note from below screenshot, only the default VLAN (vlan number 1) is configured:\n\n\n\n\n\n\n\n\n\n\nNow, go to mputty, login or launch a new ssh into Ansible node (198.18.134.150) \n\n\n\n\n\n\nUse command \nansible-playbook vlan_provision.yml --tags \"add vlans\"\n under folder \"LTRDCN-1572\" as shown below:\n\n\n[root@rhel7-tools LTRDCN-1572]# ansible-playbook vlan_provision.yml --tags \"add vlans\"\n\n\n\n\n\nNote: If the playbook fails first time, re-run the playbook again. Make sure to save all the changes in the playbook first before executing the playbook in Ansible.\n\n\n\n\n\n\nAfter playbook is run successfully, login into leaf 3 again and check if vlan 140 and vlan 141 appears. There would also be a log message on the screen indicating a configuration change was pushed to the device:\n\n\n\n\n\n\n\n\n\n\nStep 6: Server port VLAN tasks in playbook\n\n\nNow, we have tested our first playbook with basic configuration ( adding 2 VLANS), we are now going to add more tasks in our existing playbook \u201c\nvlan_provision.yml\n\u201d\n\n\n\n\nwe will add new plays in the playbook to assign VLANs to server facing port. This time, we will configure VLAN towards the server facing ports\n\n\nGo back to \nATOM\n and \nadd\n the following plays to the \nexisting playbook\n\n\n\n\n      - name: configure server facing port to L2\n        nxos_interface:\n          interface: eth1/3\n          mode: layer2\n          provider: \"{{nxos_provider}}\"\n      - name: configure VLAN for server port\n        when: (\"101\" in inventory_hostname) or (\"103\" in inventory_hostname)\n        nxos_switchport:\n          interface: eth1/3\n          mode: access\n          access_vlan: 140\n          provider: \"{{nxos_provider}}\"\n      - name: configure VLAN for server port\n        when: (\"102\" in inventory_hostname) or (\"104\" in inventory_hostname)\n        nxos_switchport:\n          interface: eth1/3\n          mode: access\n          access_vlan: 141\n          provider: \"{{nxos_provider}}\"\n\n\n\n\nIn this new play, we used nxos module \u201c\nnxos_interface\n\u201d and \u201c\nnxos_switchport\n\u201d. \n\n\n\n\n\u201c\nnxos_interface\n\u201d provides the capability to manage the physical attributes of an interface\n\n\nIn this example, it is used to configure \u201clayer 2\u201d on interface Ethernet 1/3\n\n\n\n\n\n\n\u201c\nnxos_switchport\n\u201d provides the capability to manage the Layer 2 switchport attributes\n\n\nIn this example, it is used to configuration it is used to configure mode access on Ethernet ports 1/3\n\n\n\n\n\n\nWe used \u201c\nwhen\n\u201d argument to provide little logic of the play. \n\n\nIn our example, the playbook assign VLAN 140 on leaf1 and leaf3 switches; assign VLAN 141 on leaf2 and leaf4 switches. \n\n\n\n\n\n\nClick \u201c\nFile\n\u201d and \u201c\nSave\n\u201d will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\n\n\nStep 7: Execute playbook\n\n\nNow, we are going to execute the playbook with the command \nansible-playbook vlan_provision.yml\n\n\n\n\nOn MTPuttY, log back into (or launch a new ssh) into \u201cAnsible\u201d node.\n\n\n\n\nExecute command \nansible-playbook vlan_provision.yml\n in the \u201cLTRDCN-1572\u201d directory as shown below:\n\n\n[root@rhel7-tools LTRDCN-1572]# ansible-playbook vlan_provision.yml\n\n\n\n\n\n\n\n\n\nAfter we push the configuration, \nlogin\n to the \nleaf-3\n switch, and check if the server facing port has the access vlan configured with the below command:\n  \nshow run interface ethernet1/3\n\n\nThe output of above command is shown in below screenshot:\n\n\n\n\n\n\n\n\n\n\nCongratulation! You have created your first ansible playbook, automatically provisioned new VLANs and assigned port to new created VLANs using Ansible. Next we are going to create VXLAN Fabric using Ansible.",
            "title": "Task 2 - First Simple Ansible Playbook"
        },
        {
            "location": "/task2-first-ansible/#task-2-first-simple-ansible-playbook",
            "text": "In this section, we will create the first Ansible Playbook. In the playbook, we will configure VLANs on leaf switches, and assign VLANs to the server facing port.   You will learn create variables inside playbook, learn simple loop using \u201c with_items \u201d, learn simple logical using \u201c when \u201d and learn \u201c tags \u201d to isolate tasks from whole playbook.",
            "title": "Task 2: First Simple Ansible Playbook"
        },
        {
            "location": "/task2-first-ansible/#step-1-using-atom-text-editor",
            "text": "Open \u201cAtom\u201d text editor by double click the icon on desktop. Atom is the recommended text editor for this lab:     After opening ATOM, Click \u201cNo Never\u201d to the \u201cRegister as default atom:// URI handlre\u201d message as show below:",
            "title": "Step 1: Using \"Atom\" text Editor"
        },
        {
            "location": "/task2-first-ansible/#step-2-atom-folder",
            "text": "After opening ATOM, there should be a folder in the left pane named \u201cLTRDCN-1572.\u201d   Right click the pre-configured project folder  LTRDCN-1572  and select  New File       Name the new file  vlan_provision.yml  and hit enter. This will creat the new file:",
            "title": "Step 2: Atom folder"
        },
        {
            "location": "/task2-first-ansible/#step-3-define-variables-tasks-for-playbook",
            "text": "In this step, we are going to define the scope, variable for playbook and tasks    In the opened window for \u2018vlan_provision.yml\u2019 file, enter the below content.  NOTE: YAML is space sensitive. Hence be careful with the spaces in below section.      ---                     \n#Task 2: Simple playbook assign VLAN to server facing port\n  - hosts: leaf:jinja2_leaf\n    vars:\n      nxos_provider:\n        username: \"{{ user }}\"\n        password: \"{{ pwd }}\"\n        transport: nxapi\n        host: \"{{ inventory_hostname }}\"   \u201c hosts \u201d defines the scope of this playbook applies to all switches in group \u2018leaf\u2019 and \u2018jinja2_leaf\u2019.   Note  you can review the IP addresses of the three (2) \u201cleaf\u201d and one (1) \u201cjinja2_leaf\u201d in the \u201c hosts \u201d file (configured in previous steps).  The IP addresses are:  jinja2_leaf: 198.18.4.104  leaf: 198.18.4.101  leaf: 198.18.4.103      \u201c vars \u201d defines one variable \u201c nxos_provider \u201d that will be used in this playbook.   \u201c nxos_provider \u201d is variable that includes all value that will be used for connection and autnentication.   This variable will be referenced in playbook via \u201c provider \u201d argument.",
            "title": "Step 3: Define variables, tasks for playbook"
        },
        {
            "location": "/task2-first-ansible/#step-4-vlan-tasks-in-playbook",
            "text": "Continue to add below tasks on the playbook:       tasks:\n      - name: provision VLAN\n        nxos_config:\n          lines: \"vlan {{item}}\"\n          provider: \"{{nxos_provider}}\"\n        with_items:\n          -  140\n          -  141\n        tags: add vlans   Multiple plays can be defined in one playbook under \u201c tasks \u201d, each starts with \u201c-\u201c .    This kind of play creates multiple VLANs using  nxos_config  module.    The \u201clines\u201d option is used to pass only one configuration command.  This commands must be the exact same commands as found in the device running-config.  Though one or multiple configuration commands can be configured under the \u201clines\u201d option.   For multiple, ordered set of commands can be configured under this \u201cline\u201d section.     At the end of this play, we use \u201c tags \u201d to name the play \u201c add vlans \u201d. This is useful to run a specific part of the configuration without running whole playbook.    NOTE: Formatting is extremely important when working with Ansible. Ansible playbook would return errors if the spaces are not properly aligned or formatting is not correct    Click \u201c File \u201d and \u201c Save \u201d . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.    NOTE: Once the Save button is pressed, then at the lower part of ATOM app, you will see message about connecting to Ansibe host (198.18.134.150) and saving the vlan_provision.yml file",
            "title": "Step 4: VLAN tasks in playbook"
        },
        {
            "location": "/task2-first-ansible/#step-5-execute-playbook",
            "text": "After creating the playbook, it is now time to execute the playbook.    Before executing the playbook, we will verify the leaf switch if it has any vlan configuration present on it.   Login  to leaf-3 switch using the Mputty client, or any leaf switch and execute  show vlan brief  command.    This command will show the vlans that currently exist on the leaf switch.  As you note from below screenshot, only the default VLAN (vlan number 1) is configured:      Now, go to mputty, login or launch a new ssh into Ansible node (198.18.134.150)     Use command  ansible-playbook vlan_provision.yml --tags \"add vlans\"  under folder \"LTRDCN-1572\" as shown below:  [root@rhel7-tools LTRDCN-1572]# ansible-playbook vlan_provision.yml --tags \"add vlans\"   Note: If the playbook fails first time, re-run the playbook again. Make sure to save all the changes in the playbook first before executing the playbook in Ansible.    After playbook is run successfully, login into leaf 3 again and check if vlan 140 and vlan 141 appears. There would also be a log message on the screen indicating a configuration change was pushed to the device:",
            "title": "Step 5: Execute playbook"
        },
        {
            "location": "/task2-first-ansible/#step-6-server-port-vlan-tasks-in-playbook",
            "text": "Now, we have tested our first playbook with basic configuration ( adding 2 VLANS), we are now going to add more tasks in our existing playbook \u201c vlan_provision.yml \u201d   we will add new plays in the playbook to assign VLANs to server facing port. This time, we will configure VLAN towards the server facing ports  Go back to  ATOM  and  add  the following plays to the  existing playbook         - name: configure server facing port to L2\n        nxos_interface:\n          interface: eth1/3\n          mode: layer2\n          provider: \"{{nxos_provider}}\"\n      - name: configure VLAN for server port\n        when: (\"101\" in inventory_hostname) or (\"103\" in inventory_hostname)\n        nxos_switchport:\n          interface: eth1/3\n          mode: access\n          access_vlan: 140\n          provider: \"{{nxos_provider}}\"\n      - name: configure VLAN for server port\n        when: (\"102\" in inventory_hostname) or (\"104\" in inventory_hostname)\n        nxos_switchport:\n          interface: eth1/3\n          mode: access\n          access_vlan: 141\n          provider: \"{{nxos_provider}}\"  In this new play, we used nxos module \u201c nxos_interface \u201d and \u201c nxos_switchport \u201d.    \u201c nxos_interface \u201d provides the capability to manage the physical attributes of an interface  In this example, it is used to configure \u201clayer 2\u201d on interface Ethernet 1/3    \u201c nxos_switchport \u201d provides the capability to manage the Layer 2 switchport attributes  In this example, it is used to configuration it is used to configure mode access on Ethernet ports 1/3    We used \u201c when \u201d argument to provide little logic of the play.   In our example, the playbook assign VLAN 140 on leaf1 and leaf3 switches; assign VLAN 141 on leaf2 and leaf4 switches.     Click \u201c File \u201d and \u201c Save \u201d will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.",
            "title": "Step 6: Server port VLAN tasks in playbook"
        },
        {
            "location": "/task2-first-ansible/#step-7-execute-playbook",
            "text": "Now, we are going to execute the playbook with the command  ansible-playbook vlan_provision.yml   On MTPuttY, log back into (or launch a new ssh) into \u201cAnsible\u201d node.   Execute command  ansible-playbook vlan_provision.yml  in the \u201cLTRDCN-1572\u201d directory as shown below:  [root@rhel7-tools LTRDCN-1572]# ansible-playbook vlan_provision.yml     After we push the configuration,  login  to the  leaf-3  switch, and check if the server facing port has the access vlan configured with the below command:\n   show run interface ethernet1/3  The output of above command is shown in below screenshot:      Congratulation! You have created your first ansible playbook, automatically provisioned new VLANs and assigned port to new created VLANs using Ansible. Next we are going to create VXLAN Fabric using Ansible.",
            "title": "Step 7: Execute playbook"
        }
    ]
}