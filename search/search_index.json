{
    "docs": [
        {
            "location": "/",
            "text": "LTRDCN-1572\n\n\nWelcome to Cisco Live LTRDCN-1572: VXLAN EVPN Fabric and automation using Ansible.\n\n\nFor full documentation visit \nCisco Live\n.\n\n\nSpeakers\n\n\n\n\nFaisal Chaudhry \nPrincipal Architect, Cisco Advanced Services\n\n\nLei Tian \nSolutions Architect, Cisco Advanced Services",
            "title": "Home"
        },
        {
            "location": "/#ltrdcn-1572",
            "text": "Welcome to Cisco Live LTRDCN-1572: VXLAN EVPN Fabric and automation using Ansible.  For full documentation visit  Cisco Live .",
            "title": "LTRDCN-1572"
        },
        {
            "location": "/#speakers",
            "text": "Faisal Chaudhry  Principal Architect, Cisco Advanced Services  Lei Tian  Solutions Architect, Cisco Advanced Services",
            "title": "Speakers"
        },
        {
            "location": "/intro/",
            "text": "VXLAN\n\n\nVXLAN stands for Virtual Extensible Local Area Network. VXLAN is a L2 overlay scheme on top of L3 network or we can say it is a L2 in layer 3 tunnel. It runs over the existing networks and provides the means to stretch the L2 network. Only VMs within the same VXLAN segment can communicate with each other. Each VXLAN segment is identified by a 24 bit segment ID called \u201cVXLAN Network Identifier (VNI)\u201d.  This help overcome 4094 VLAN scale limitation and able to extend it to 224 segments.\n\n\nVXLAN uses BGP as its control plane for Overlay. It makes it forwarding decisions at VTEPs (Virtual tunnel end points) for layer-2 and layer-3. Forwarding happens based on MAC or IP learnt via control plane (MP-BGP EVPN) . VXLAN uses IGP, PIM and BGP as its underlay in the fabric. \n\n\nBelow are some of the terminologies that will be used in the lab:\n\n\n\n\nVNI / VNID\n \u2013 VXLAN Network Identifier. This replaces VLAN ID \n\n\nVTEP\n \u2013 VXLAN Tunnel End Point.\n\n\nThis is the end point where the box performs VXLAN encap / decap\nThis could be physical HW (Nexus9k) or Virtual (Nexus 1000v, Nexus 9000v)\n\n\n\n\n\n\nVXLAN Segment\n -  The resulting layer 2 overlay network\n\n\nVXLAN Gateway\n \u2013 It is a device that forwards traffic between VXLANS. It can be both L2 and L3 forwarding\n\n\nNVE\n \u2013 Network Virtualization Edge\n\n\nNVE is tunnel interface. It represents VTEP\n\n\n\n\n\n\n\n\nAnsible\n\n\nAnsible is an agentless open source software that can be used for configuration management, deployment and orchestration of deployment. The scripts in Ansible are called playbooks; playbook is in YAML format that was desgiened to be easy for humans to read and write. Playbooks include one or more plays, each play include one or more tasks. Each task is associated with one module, which is what gets executed in the playbook. Modules are python scripts that ship with Ansible installation. During the lab, you will be introduced to multiple NXOS modules and ansible template module. \n\n\nYou can find all Ansible modules documentation at below url:\n\nhttp://docs.ansible.com/ansible/latest/list_of_all_modules.html\n\n\nBelow are some of the terminologies that will be used in the lab:\n\n\n\n\nHost\n: remote machines that Ansible manages  \n\n\nGroup\n: several hosts that can be configured together and share common verables \n\n\nInventory\n: file descripts hosts and groups in Ansible.\n\n\nVariable\n: names of value (int, str, dic, list) referenced in playbook or template\n\n\nYAML\n: data format for Playbook or Variables in Ansible \n\n\nPlaybook\n: the script to orchestrate, automate, deploy system in Ansible. One playbook can include multiple plays. \n\n\nRoles\n: group of tasks, templates to implement specific behavior\n\n\nJinja2\n: a Python based tempting language\n\n\n\n\n\n\nAbout this lab\n\n\nAs a standardized overlay technology, multiple vendors have adopted VXLAN as datacenter solution to provide scalability and allow layer 2 across IP network. MP-BPG EVPN as VXLAN control plane protocol provides a robust scalable solution to overcome the limitation in VXLAN flood and learn mode.\n\n\nAs an open source automation tool, Ansible provides the same framework for network administrators to automate network infrastructure as the rest IT organization. \n\n\nThis lab demostates the possibility of using Ansible to automate datacenter VXLAN fabric day 1 provisiong and day 2 operations. \n\n\nLab Flow\n\n\nLab guide will walk the attendees through the below activities:\n\n\n\n\nAnsible installation \n\n\nAnsible playbook \n\n\nDay 1 automation using Ansible \n\n\nDay 2 automation using Ansible \n\n\nDay 0 automation \n\n\nL4-L7 Service insertion\n\n\n\n\nLab Access\n\n\nBelow table provides the IP addresses and credentials for the devices used in this lab: \n\n\n\n\n\n\n\n\nSpine-1\n\n\n198.18.133.33:1030\n\n\nadmin/C1sco12345\n\n\n\n\n\n\n\n\n\n\nSpine-2\n\n\n198.18.133.33:1040\n\n\nadmin/C1sco12345\n\n\n\n\n\n\nLeaf-1\n\n\n198.18.133.33:1050\n\n\nadmin/C1sco12345\n\n\n\n\n\n\nLeaf-3\n\n\n198.18.133.33:1070\n\n\nadmin/C1sco12345\n\n\n\n\n\n\nLeaf-4\n\n\n198.18.1333.33:1080\n\n\nadmin/C1sco12345\n\n\n\n\n\n\nServer-1\n\n\n198.18.134.50\n\n\nroot/C1sco12345\n\n\n\n\n\n\nServer-3\n\n\n198.18.134.52\n\n\nroot/C1sco12345\n\n\n\n\n\n\nServer-4\n\n\n198.18.134.53\n\n\nroot/C1sco12345\n\n\n\n\n\n\nAnsible Server\n\n\n198.18.134.150\n\n\nroot/C1sco12345\n\n\n\n\n\n\nDCNM\n\n\n198.18.134.200\n\n\nadmin/C1sco12345\n\n\n\n\n\n\nF5\n\n\n198.18.4.10\n\n\nroot/default\n\n\n\n\n\n\nRemote Workstation\n\n\n198.18.133.36\n\n\ndemouser/C1sco12345\n\n\n\n\n\n\n\n\nLab topology\n\n\nBelow picture shows the lab topology:",
            "title": "Introduction"
        },
        {
            "location": "/intro/#vxlan",
            "text": "VXLAN stands for Virtual Extensible Local Area Network. VXLAN is a L2 overlay scheme on top of L3 network or we can say it is a L2 in layer 3 tunnel. It runs over the existing networks and provides the means to stretch the L2 network. Only VMs within the same VXLAN segment can communicate with each other. Each VXLAN segment is identified by a 24 bit segment ID called \u201cVXLAN Network Identifier (VNI)\u201d.  This help overcome 4094 VLAN scale limitation and able to extend it to 224 segments.  VXLAN uses BGP as its control plane for Overlay. It makes it forwarding decisions at VTEPs (Virtual tunnel end points) for layer-2 and layer-3. Forwarding happens based on MAC or IP learnt via control plane (MP-BGP EVPN) . VXLAN uses IGP, PIM and BGP as its underlay in the fabric.   Below are some of the terminologies that will be used in the lab:   VNI / VNID  \u2013 VXLAN Network Identifier. This replaces VLAN ID   VTEP  \u2013 VXLAN Tunnel End Point.  This is the end point where the box performs VXLAN encap / decap\nThis could be physical HW (Nexus9k) or Virtual (Nexus 1000v, Nexus 9000v)    VXLAN Segment  -  The resulting layer 2 overlay network  VXLAN Gateway  \u2013 It is a device that forwards traffic between VXLANS. It can be both L2 and L3 forwarding  NVE  \u2013 Network Virtualization Edge  NVE is tunnel interface. It represents VTEP",
            "title": "VXLAN"
        },
        {
            "location": "/intro/#ansible",
            "text": "Ansible is an agentless open source software that can be used for configuration management, deployment and orchestration of deployment. The scripts in Ansible are called playbooks; playbook is in YAML format that was desgiened to be easy for humans to read and write. Playbooks include one or more plays, each play include one or more tasks. Each task is associated with one module, which is what gets executed in the playbook. Modules are python scripts that ship with Ansible installation. During the lab, you will be introduced to multiple NXOS modules and ansible template module.   You can find all Ansible modules documentation at below url: http://docs.ansible.com/ansible/latest/list_of_all_modules.html  Below are some of the terminologies that will be used in the lab:   Host : remote machines that Ansible manages    Group : several hosts that can be configured together and share common verables   Inventory : file descripts hosts and groups in Ansible.  Variable : names of value (int, str, dic, list) referenced in playbook or template  YAML : data format for Playbook or Variables in Ansible   Playbook : the script to orchestrate, automate, deploy system in Ansible. One playbook can include multiple plays.   Roles : group of tasks, templates to implement specific behavior  Jinja2 : a Python based tempting language",
            "title": "Ansible"
        },
        {
            "location": "/intro/#about-this-lab",
            "text": "As a standardized overlay technology, multiple vendors have adopted VXLAN as datacenter solution to provide scalability and allow layer 2 across IP network. MP-BPG EVPN as VXLAN control plane protocol provides a robust scalable solution to overcome the limitation in VXLAN flood and learn mode.  As an open source automation tool, Ansible provides the same framework for network administrators to automate network infrastructure as the rest IT organization.   This lab demostates the possibility of using Ansible to automate datacenter VXLAN fabric day 1 provisiong and day 2 operations.",
            "title": "About this lab"
        },
        {
            "location": "/intro/#lab-flow",
            "text": "Lab guide will walk the attendees through the below activities:   Ansible installation   Ansible playbook   Day 1 automation using Ansible   Day 2 automation using Ansible   Day 0 automation   L4-L7 Service insertion",
            "title": "Lab Flow"
        },
        {
            "location": "/intro/#lab-access",
            "text": "Below table provides the IP addresses and credentials for the devices used in this lab:      Spine-1  198.18.133.33:1030  admin/C1sco12345      Spine-2  198.18.133.33:1040  admin/C1sco12345    Leaf-1  198.18.133.33:1050  admin/C1sco12345    Leaf-3  198.18.133.33:1070  admin/C1sco12345    Leaf-4  198.18.1333.33:1080  admin/C1sco12345    Server-1  198.18.134.50  root/C1sco12345    Server-3  198.18.134.52  root/C1sco12345    Server-4  198.18.134.53  root/C1sco12345    Ansible Server  198.18.134.150  root/C1sco12345    DCNM  198.18.134.200  admin/C1sco12345    F5  198.18.4.10  root/default    Remote Workstation  198.18.133.36  demouser/C1sco12345",
            "title": "Lab Access"
        },
        {
            "location": "/intro/#lab-topology",
            "text": "Below picture shows the lab topology:",
            "title": "Lab topology"
        },
        {
            "location": "/task1-ansible-node/",
            "text": "Your first task will be to build an Ansible node on a server running redhat CentOS operating system.  At the end of this task, you will have a fully operational Ansible node.\n\n\nStep 1: Connect to lab using anyconnect VPN\n\n\nYou will connect to \ndcloud-sjc-anyconnect.cisco.com\n using Cisco VPN AnyConnect client, as shown in below picture, with the username and password provided by the lab admin.\n\n\nNote:\n lab admin will furnish the credentials information to the participant.  If you don't have this information please ask the lab speakers.\n\n\n\n\nStep 2: Enter VPN credentials\n\n\nAfter prompted for credentials, use the credentials provided by the lab admin.    \n\n\u2022   Below is an example of user logging into POD1\n\n\n\n\n\n\nHit accept when the prompt appears to accept the VPN connection login    \n\n\n\n\n\n\nStep 3: RDP to workstation\n\n\nIn this step, you will connect to the workstation with RDP client on your machines.  Use below details for this RDP session:\n\n\n\n\nWorkstation: \n198.18.133.36\n\n\nUsername: \ndcloud\\demouser\n\n\nPassword: \nC1sco12345\n\n\n\n\nBelow screenshot is only an example for this RDP connection:\n\n\n\n\nStep 4: MTputty\n\n\nOnce you have the RDP session to the remote workstation, then you will use MTputty client to connect to all devices in this lab.  \n\n\nMTputty is already installed on the Desktop of the workstation where you connected using RDP.  Run this application by clicking on the icon on the desktop:\n\n\n\n\nStep 5: SSH into Ansible node\n\n\nSSH into Ansible node (198.18.134.150) by double clicking the Ansible icon on the left pan with username \nroot\n and password \nC1sco12345\n\n\n\n\nStep 6: Verify Python\n\n\nOnce successfully SSH into the ansible node, the very first thing we are going to do after logging into Ansible server is verify the python version by running \npython --version\n command - as shown below:\n\n\n[root@rhel7-tools ~]# python --version\nPython 2.7.5\n\n\n\nIt is an important step as we need minimum 2.7.5 version of python in order to install some features for ansbile.  The output of above command confirms this version. \n\n\nAnsible can be run from any machine with Python 2 (versions 2.6 or 2.7) or Python 3 (versions 3.5 and higher) installed. \n\n\n\n\nStep 7: Install PIP\n\n\nAfter verifying we have the minimum version of python installed, we are now going to Install PIP python package using \neasy_install pip\n command as shown below:\n\n\n[root@rhel7-tools ~]# easy_install pip\nSearching for pip\nBest match: pip 8.1.1\nAdding pip 8.1.1 to easy-install.pth file\nInstalling pip script to /usr/bin\nInstalling pip3.5 script to /usr/bin\nInstalling pip3 script to /usr/bin\n\nUsing /usr/lib/python2.7/site-packages\nProcessing dependencies for pip\nFinished processing dependencies for pip\n\n\n\nBelow screenshot shows the execution of above command:\n\n\n\n\nNext, update pip to latest version by executing below command:\n\n\n[root@rhel7-tools ~]# pip install --upgrade pip\n\n\n\nBelow screenshot shows the exection of above command:\n\n\n\n\nAfter installing PIP package, we are going to add the relevant packages that are needed for this Ansible based VXLAN lab. Below are the packages required for this lab.  \n\n\n\n\nParamiko\n\n\nPyYAML\n\n\nJinj2\n\n\nHttplib2\n\n\n\n\nRun below command to install these packages:\n\n\n[root@rhel7-tools ~]# pip install paramiko PyYAML jinja2 httplib2\n\n\n\nBelow screenshot shows the execution of above command:\n\n\n\n\nAs a final step, we are going to install Ansible on this RHEL. Once the install is initiated with the below command, it will take few minutes for it to download and install.\n\n\n[root@rhel7-tools ~]# pip install ansible==2.5.0\n\n\n\nBelow screenshot shows the execution of above command:\n\n\n\n\nStep 8: Verify Ansible\n\n\nAfter installation is complete, check Ansible version by executing command \nansible --version\n, as shown below:\n\n\n[root@rhel7-tools ~]# ansible --version \nansible 2.5.0\n  config file = None\n  configured module search path = [u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']\n  ansible python module location = /usr/lib/python2.7/site-packages/ansible\n  executable location = /usr/bin/ansible\n  python version = 2.7.5 (default, Apr  9 2015, 11:03:32) [GCC 4.8.3 20140911 (Red Hat 4.8.3-9)]\n[root@rhel7-tools ~]#\n\n\n\nStep 9: Create Ansible Inventory\n\n\nNow, we are going to create inventory, host variables and Configuration file. This is important as Ansible works  against multiple systems in the system by selecting portions of systems listed in Ansible inventory. Similarly, configuration settings in Ansible are adjustable via configuration file.\n\n\nCreate folder named LTRDCN-1572 as working environment and verify that it\u2019s empty:\n\n\n[root@rhel7-tools ~]# mkdir LTRDCN-1572 && cd LTRDCN-1572\n[root@rhel7-tools LTRDCN-1572]# ls\n\n\n\nBelow screenshot shows the execution of above command:\n\n\n\n\nNext:\n\n\n\n\nCreate Ansible inventory file to include Spine and Leaf switches. \n\n\nBy default Ansible has inventory file saved in location /etc/ansible/hosts. \n\n\n\n\nIn this lab we will create hosts file in the working environment. Use \u2018vi\u2019 to create inventory file \u2018hosts\u2019 as shown below:\n\n\n[root@rhel7-tools LTRDCN-1572]# vi hosts\n\n\n\n\n\n\n\nAt the bottom of the inventory file, insert (type \ni\n) the following lines:\n\n\n#define global variables, groups and host variables\n[all:vars]\nansible_connection = local\nuser=admin\npwd=C1sco12345\ngather_fact=no\n[jinja2_spine]\n198.18.4.202\n[jinja2_leaf]\n198.18.4.104\n[spine]\n198.18.4.201\n[leaf]\n198.18.4.101\n198.18.4.103\n[server]\n198.18.134.50 eth1=172.21.140.10 gw=172.21.140.1\n198.18.134.52 eth1=172.21.140.11 gw=172.21.140.1\n198.18.134.53 eth1=172.21.141.11 gw=172.21.141.1\n[f5]\n198.18.4.10\n\n\n\n\n\n\n\nQuit and save vi editor by press \nEsc\n key and then type \n:wq!\n  \n\n\nBelow screenshot shows the execution of above command:\n\n\n\n\n\n\n\nCreate Ansible config (ansible.cfg) file via vi editor pointing to local \u2018hosts\u2019 file for inventory. Use \nvi ansible.cfg\n to create this file as shown below:\n\n\n[root@rhel7-tools LTRDCN-1572]# vi ansible.cfg\n\n\n\n\n\n\n\nAt the bottom of the ansible.cfg file, insert the below lines (by typing \ni\n):\n\n\n[defaults]\ninventory = hosts\nhost_key_checking = false\nrecord_host_key = true\nstdout_callback = debug\n\n\n\n\n\n\n\nQuit and save vi editor by press \nEsc\n key and then type \n:wq!\n  \n\n\n\n\n\n\nDo \nls\n to verify the file that you just created under project folder LTRDCN-1572.   \n\n\nBelow screenshot shows the execution of above command:\n\n\n\n\n\n\n\nCreate host variable folder named host_vars in folder LTRDCN-1572. Host varilables can be placed in different places in Ansible. In this lab, we will use host_vars for host variables:\n\n\n[root@rhel7-tools LTRDCN-1572]# mkdir host_vars && cd host_vars\n\n\n\n\n\n\n\nCreate host variable file for each host in inventory. Use \nvi\n to create following file: \n\n\n[root@rhel7-tools host_vars]# vi 198.18.4.101.yml\n\n\n\nIn the file: add below data (by typing \ni\n):\n\n\n\n\n\n\n---\n  hostname: leaf_1\n  loopback0: 192.168.0.8\n  loopback1: 192.168.0.18\n  router_id: 192.168.0.8\n\n\n\n\n\n\n\n\nQuit and save vi editor by pressing \nEsc\n key and then type \n:wq!\n   \n\n\n\n\n\n\nCreate a new host variable file for next host in inventory. Use \nvi\n to create following file: \n\n\n[root@rhel7-tools host_vars]# vi 198.18.4.103.yml\n\n\n\nIn the file: add below data (by typing \ni\n):\n\n\n\n\n\n\n---\n  hostname: leaf_3\n  loopback0: 192.168.0.10\n  loopback1: 192.168.0.110\n  router_id: 192.168.0.10\n\n\n\n\n\n\n\n\nQuit and save vi editor by pressing \nEsc\n key and then type \n:wq!\n   \n\n\n\n\n\n\nCreate a new host variable file for next host in inventory. Use \nvi\n to create following file: \n\n\n[root@rhel7-tools host_vars]# vi 198.18.4.104.yml\n\n\n\nIn the file: add below data (by typing \ni\n):\n\n\n\n\n\n\n---\n  hostname: leaf_4\n  loopback0: 192.168.0.11\n  loopback1: 192.168.0.111\n  router_id: 192.168.0.11\n\n\n\n\n\n\n\n\nQuit and save vi editor by pressing \nEsc\n key and then type \n:wq!\n   \n\n\n\n\n\n\nCreate a new host variable file for next host in inventory. Use \nvi\n to create following file: \n\n\n[root@rhel7-tools host_vars]# vi 198.18.4.201.yml\n\n\n\nIn the file: add below data (by typing \ni\n):\n\n\n\n\n\n\n---\n  hostname: spine-1\n  loopback0: 192.168.0.6\n  loopback1: 192.168.0.100\n  router_id: 192.168.0.6\n\n\n\n\n\n\n\n\nQuit and save vi editor by press \nEsc\n key and then type \n:wq!\n \n\n\n\n\n\n\nCreate a new host variable file for next host in inventory. Use \nvi\n to create following file: \n\n\n[root@rhel7-tools host_vars]# vi 198.18.4.202.yml\n\n\n\nIn the file: add below data (by typing \ni\n):\n\n\n\n\n\n\n---\n  hostname: spine-2\n  loopback0: 192.168.0.7 \n  loopback1: 192.168.0.100\n  router_id: 192.168.0.7\n\n\n\n\n\n\nQuit and save vi editor by press \nEsc\n key and then type \n:wq!\n \n\n\n\n\nStep 10: Ansible role structure\n\n\nRole is very useful technique to manage a set of playbooks in Ansible. \nIn this lab, we will use two different playbooks to manage configuration for Spine and Leaf switches. \n\n\nWe will use role structure and manage the two plays into single playbook. \nA role directory structure contains several directories of defaults, vars, files, handlers, meta, tasks and templates. \n\n\nIn this lab:\n\n\n\n\nwe will use vars, templates and tasks folders \n/vars\n \n\n\nmain.yml file in vars folder contains dictionary of variables for this role \n/tasks\n\n\nmain.yml file in tasks folder contains the Ansible playbook for this role\n\n\n\n\nTo proceed further in the new:\n\n\n\u2022   Create roles directory in folder LTRDCN-1572 by issuing below commands:\n\n\n[root@rhel7-tools LTRDCN-1572]# cd /root/LTRDCN-1572\n[root@rhel7-tools LTRDCN-1572]# mkdir roles\n\n\n\nThis will be used in the lab later.",
            "title": "Task 1 - Prepare Ansible node"
        },
        {
            "location": "/task1-ansible-node/#step-1-connect-to-lab-using-anyconnect-vpn",
            "text": "You will connect to  dcloud-sjc-anyconnect.cisco.com  using Cisco VPN AnyConnect client, as shown in below picture, with the username and password provided by the lab admin.  Note:  lab admin will furnish the credentials information to the participant.  If you don't have this information please ask the lab speakers.",
            "title": "Step 1: Connect to lab using anyconnect VPN"
        },
        {
            "location": "/task1-ansible-node/#step-2-enter-vpn-credentials",
            "text": "After prompted for credentials, use the credentials provided by the lab admin.     \n\u2022   Below is an example of user logging into POD1    Hit accept when the prompt appears to accept the VPN connection login",
            "title": "Step 2: Enter VPN credentials"
        },
        {
            "location": "/task1-ansible-node/#step-3-rdp-to-workstation",
            "text": "In this step, you will connect to the workstation with RDP client on your machines.  Use below details for this RDP session:   Workstation:  198.18.133.36  Username:  dcloud\\demouser  Password:  C1sco12345   Below screenshot is only an example for this RDP connection:",
            "title": "Step 3: RDP to workstation"
        },
        {
            "location": "/task1-ansible-node/#step-4-mtputty",
            "text": "Once you have the RDP session to the remote workstation, then you will use MTputty client to connect to all devices in this lab.    MTputty is already installed on the Desktop of the workstation where you connected using RDP.  Run this application by clicking on the icon on the desktop:",
            "title": "Step 4: MTputty"
        },
        {
            "location": "/task1-ansible-node/#step-5-ssh-into-ansible-node",
            "text": "SSH into Ansible node (198.18.134.150) by double clicking the Ansible icon on the left pan with username  root  and password  C1sco12345",
            "title": "Step 5: SSH into Ansible node"
        },
        {
            "location": "/task1-ansible-node/#step-6-verify-python",
            "text": "Once successfully SSH into the ansible node, the very first thing we are going to do after logging into Ansible server is verify the python version by running  python --version  command - as shown below:  [root@rhel7-tools ~]# python --version\nPython 2.7.5  It is an important step as we need minimum 2.7.5 version of python in order to install some features for ansbile.  The output of above command confirms this version.   Ansible can be run from any machine with Python 2 (versions 2.6 or 2.7) or Python 3 (versions 3.5 and higher) installed.",
            "title": "Step 6: Verify Python"
        },
        {
            "location": "/task1-ansible-node/#step-7-install-pip",
            "text": "After verifying we have the minimum version of python installed, we are now going to Install PIP python package using  easy_install pip  command as shown below:  [root@rhel7-tools ~]# easy_install pip\nSearching for pip\nBest match: pip 8.1.1\nAdding pip 8.1.1 to easy-install.pth file\nInstalling pip script to /usr/bin\nInstalling pip3.5 script to /usr/bin\nInstalling pip3 script to /usr/bin\n\nUsing /usr/lib/python2.7/site-packages\nProcessing dependencies for pip\nFinished processing dependencies for pip  Below screenshot shows the execution of above command:   Next, update pip to latest version by executing below command:  [root@rhel7-tools ~]# pip install --upgrade pip  Below screenshot shows the exection of above command:   After installing PIP package, we are going to add the relevant packages that are needed for this Ansible based VXLAN lab. Below are the packages required for this lab.     Paramiko  PyYAML  Jinj2  Httplib2   Run below command to install these packages:  [root@rhel7-tools ~]# pip install paramiko PyYAML jinja2 httplib2  Below screenshot shows the execution of above command:   As a final step, we are going to install Ansible on this RHEL. Once the install is initiated with the below command, it will take few minutes for it to download and install.  [root@rhel7-tools ~]# pip install ansible==2.5.0  Below screenshot shows the execution of above command:",
            "title": "Step 7: Install PIP"
        },
        {
            "location": "/task1-ansible-node/#step-8-verify-ansible",
            "text": "After installation is complete, check Ansible version by executing command  ansible --version , as shown below:  [root@rhel7-tools ~]# ansible --version \nansible 2.5.0\n  config file = None\n  configured module search path = [u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']\n  ansible python module location = /usr/lib/python2.7/site-packages/ansible\n  executable location = /usr/bin/ansible\n  python version = 2.7.5 (default, Apr  9 2015, 11:03:32) [GCC 4.8.3 20140911 (Red Hat 4.8.3-9)]\n[root@rhel7-tools ~]#",
            "title": "Step 8: Verify Ansible"
        },
        {
            "location": "/task1-ansible-node/#step-9-create-ansible-inventory",
            "text": "Now, we are going to create inventory, host variables and Configuration file. This is important as Ansible works  against multiple systems in the system by selecting portions of systems listed in Ansible inventory. Similarly, configuration settings in Ansible are adjustable via configuration file.  Create folder named LTRDCN-1572 as working environment and verify that it\u2019s empty:  [root@rhel7-tools ~]# mkdir LTRDCN-1572 && cd LTRDCN-1572\n[root@rhel7-tools LTRDCN-1572]# ls  Below screenshot shows the execution of above command:   Next:   Create Ansible inventory file to include Spine and Leaf switches.   By default Ansible has inventory file saved in location /etc/ansible/hosts.    In this lab we will create hosts file in the working environment. Use \u2018vi\u2019 to create inventory file \u2018hosts\u2019 as shown below:  [root@rhel7-tools LTRDCN-1572]# vi hosts    At the bottom of the inventory file, insert (type  i ) the following lines:  #define global variables, groups and host variables\n[all:vars]\nansible_connection = local\nuser=admin\npwd=C1sco12345\ngather_fact=no\n[jinja2_spine]\n198.18.4.202\n[jinja2_leaf]\n198.18.4.104\n[spine]\n198.18.4.201\n[leaf]\n198.18.4.101\n198.18.4.103\n[server]\n198.18.134.50 eth1=172.21.140.10 gw=172.21.140.1\n198.18.134.52 eth1=172.21.140.11 gw=172.21.140.1\n198.18.134.53 eth1=172.21.141.11 gw=172.21.141.1\n[f5]\n198.18.4.10    Quit and save vi editor by press  Esc  key and then type  :wq!     Below screenshot shows the execution of above command:    Create Ansible config (ansible.cfg) file via vi editor pointing to local \u2018hosts\u2019 file for inventory. Use  vi ansible.cfg  to create this file as shown below:  [root@rhel7-tools LTRDCN-1572]# vi ansible.cfg    At the bottom of the ansible.cfg file, insert the below lines (by typing  i ):  [defaults]\ninventory = hosts\nhost_key_checking = false\nrecord_host_key = true\nstdout_callback = debug    Quit and save vi editor by press  Esc  key and then type  :wq!       Do  ls  to verify the file that you just created under project folder LTRDCN-1572.     Below screenshot shows the execution of above command:    Create host variable folder named host_vars in folder LTRDCN-1572. Host varilables can be placed in different places in Ansible. In this lab, we will use host_vars for host variables:  [root@rhel7-tools LTRDCN-1572]# mkdir host_vars && cd host_vars    Create host variable file for each host in inventory. Use  vi  to create following file:   [root@rhel7-tools host_vars]# vi 198.18.4.101.yml  In the file: add below data (by typing  i ):    ---\n  hostname: leaf_1\n  loopback0: 192.168.0.8\n  loopback1: 192.168.0.18\n  router_id: 192.168.0.8    Quit and save vi editor by pressing  Esc  key and then type  :wq!        Create a new host variable file for next host in inventory. Use  vi  to create following file:   [root@rhel7-tools host_vars]# vi 198.18.4.103.yml  In the file: add below data (by typing  i ):    ---\n  hostname: leaf_3\n  loopback0: 192.168.0.10\n  loopback1: 192.168.0.110\n  router_id: 192.168.0.10    Quit and save vi editor by pressing  Esc  key and then type  :wq!        Create a new host variable file for next host in inventory. Use  vi  to create following file:   [root@rhel7-tools host_vars]# vi 198.18.4.104.yml  In the file: add below data (by typing  i ):    ---\n  hostname: leaf_4\n  loopback0: 192.168.0.11\n  loopback1: 192.168.0.111\n  router_id: 192.168.0.11    Quit and save vi editor by pressing  Esc  key and then type  :wq!        Create a new host variable file for next host in inventory. Use  vi  to create following file:   [root@rhel7-tools host_vars]# vi 198.18.4.201.yml  In the file: add below data (by typing  i ):    ---\n  hostname: spine-1\n  loopback0: 192.168.0.6\n  loopback1: 192.168.0.100\n  router_id: 192.168.0.6    Quit and save vi editor by press  Esc  key and then type  :wq!      Create a new host variable file for next host in inventory. Use  vi  to create following file:   [root@rhel7-tools host_vars]# vi 198.18.4.202.yml  In the file: add below data (by typing  i ):    ---\n  hostname: spine-2\n  loopback0: 192.168.0.7 \n  loopback1: 192.168.0.100\n  router_id: 192.168.0.7   Quit and save vi editor by press  Esc  key and then type  :wq!",
            "title": "Step 9: Create Ansible Inventory"
        },
        {
            "location": "/task1-ansible-node/#step-10-ansible-role-structure",
            "text": "Role is very useful technique to manage a set of playbooks in Ansible. \nIn this lab, we will use two different playbooks to manage configuration for Spine and Leaf switches.   We will use role structure and manage the two plays into single playbook. \nA role directory structure contains several directories of defaults, vars, files, handlers, meta, tasks and templates.   In this lab:   we will use vars, templates and tasks folders  /vars    main.yml file in vars folder contains dictionary of variables for this role  /tasks  main.yml file in tasks folder contains the Ansible playbook for this role   To proceed further in the new:  \u2022   Create roles directory in folder LTRDCN-1572 by issuing below commands:  [root@rhel7-tools LTRDCN-1572]# cd /root/LTRDCN-1572\n[root@rhel7-tools LTRDCN-1572]# mkdir roles  This will be used in the lab later.",
            "title": "Step 10: Ansible role structure"
        },
        {
            "location": "/task2-first-ansible/",
            "text": "In this section, we will create the first Ansible Playbook. In the playbook, we will configure VLANs on leaf switches, and assign VLANs to the server facing port. \n\n\nYou will learn create variables inside playbook, learn simple loop using \u201c\nwith_items\n\u201d, learn simple logical using \u201c\nwhen\n\u201d and learn \u201c\ntags\n\u201d to isolate tasks from whole playbook. \n\n\n\n\nStep 1: Using \"Atom\" text Editor\n\n\n\n\n\n\nOpen \u201cAtom\u201d text editor by double click the icon on desktop. Atom is the recommended text editor for this lab:\n\n\n\n\n\n\n\n\nAfter opening ATOM, \nClick\n \nNo, Never\n to the \u201cRegister as default atom:// URI handlre\u201d message as show below:\n\n\n\n\n\n\n\n\n\n\nStep 2: Atom folder\n\n\n\n\nAfter opening ATOM, there should be a folder in the left pane named \u201cLTRDCN-1572.\u201d\n\n\n\n\nRight click\n the pre-configured project folder \nLTRDCN-1572\n and select \nNew File\n \n\n\n\n\n\n\n\n\nName the new file \nvlan_provision.yml\n and hit enter. This will create the new file:\n\n\n\n\n\n\n\n\nAlso, on the lower bar of the ATOM, verify that  file grammar of \nYAML\n is selected instead of default \"\nPlain Text\n\".  If \nYAML\n is not selected, then you should \nchoose\n it from the listed options.\n\n\n\n\n\n\n\n\nStep 3: Define variables, tasks for playbook\n\n\nIn this step, we are going to define the scope, variable for playbook and tasks \n\n\n\n\nIn the opened window for \u2018vlan_provision.yml\u2019 file, enter the below content.\n\n\nNOTE: YAML is space sensitive. Hence be careful with the spaces in below section. \n\n\n\n\n\n\n\n\n---                     \n#Task 2: Simple playbook assign VLAN to server facing port\n  - hosts: leaf:jinja2_leaf\n    vars:\n      nxos_provider:\n        username: \"{{ user }}\"\n        password: \"{{ pwd }}\"\n        transport: nxapi\n        host: \"{{ inventory_hostname }}\"\n\n\n\n\nNote:\n\n\n\n\n\u201c\nhosts:\n\u201d defines the scope of this playbook applies to all switches in group \u2018leaf\u2019 and \u2018jinja2_leaf\u2019 (within the \n\"hosts\"\n file created in pervious task). \n\n\nNote\n that you can review the IP addresses of the three (2) \u201cleaf\u201d and one (1) \u201cjinja2_leaf\u201d in the \u201c\nhosts\n\u201d file (configured in previous steps).  The IP addresses are:\n\n\njinja2_leaf: 198.18.4.104\n\n\nleaf: 198.18.4.101\n\n\nleaf: 198.18.4.103\n\n\n\n\n\n\n\n\n\n\n\u201c\nvars\n\u201d defines one variable \u201c\nnxos_provider\n\u201d that will be used in this playbook. \n\n\n\u201c\nnxos_provider\n\u201d is variable that includes all value that will be used for connection and autnentication. \n\n\nThis variable will be referenced in playbook via \u201c\nprovider\n\u201d argument (that will be added in next step #4).    \n\n\n\n\n\n\n\n\n\n\nStep 4: VLAN tasks in playbook\n\n\n\n\nContinue to add below tasks on the same  playbook file:\n\n\n\n\n    tasks:\n      - name: provision VLAN\n        nxos_config:\n          lines: \"vlan {{item}}\"\n          provider: \"{{nxos_provider}}\"\n        with_items:\n          -  140\n          -  141\n        tags: add vlans\n\n\n\n\nNote:\n\n\n\n\nMultiple plays can be defined in one playbook under \u201c\ntasks\n\u201d, each starts with \u201c-\u201c . \n\n\n\n\nThis kind of play creates multiple VLANs using \nnxos_config\n module. \n\n\n\n\nThe \u201clines\u201d option is used to pass only one configuration command.  This commands must be the exact same commands as found in the device running-config.\n\n\nThough one or multiple configuration commands can be configured under the \u201clines\u201d option.   For multiple, ordered set of commands can be configured under this \u201cline\u201d section.\n\n\n\n\n\n\n\n\nAt the end of this play, we use \u201c\ntags\n\u201d to name the play \u201c\nadd vlans\n\u201d. This is useful to run a specific part of the configuration without running whole playbook.\n\n\n\n\n\n\nBelow screenshot shows how playbook will look:\n\n\n\n\nNOTE: Formatting is extremely important when working with Ansible. Ansible playbook would return errors if the spaces are not properly aligned or formatting is not correct\n\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\nNOTE: Once the Save button is pressed, then at the lower part of ATOM app, you will see message about connecting to Ansibe host (198.18.134.150) and saving the vlan_provision.yml file\n\n\n\n\n\n\n\n\nStep 5: Execute playbook\n\n\nAfter creating the playbook, it is now time to execute the playbook. \n\n\n\n\nBefore executing the playbook, we will verify the leaf switch if it has any vlan configuration present on it.\n\n\n\n\nLogin\n to leaf-3 switch using the Mputty client, or any leaf switch in previous playbook (remember \nhosts:\n variable in the file), and execute \nshow vlan brief\n command. \n\n\n\n\nThis command will show the vlans that currently exist on the leaf switch.  As you note from below screenshot, only the default VLAN (vlan number 1) is configured:\n\n\n\n\n\n\n\n\n\n\nNow, go to mputty, login or launch a new ssh into Ansible node (198.18.134.150) \n\n\n\n\n\n\nUse command \nansible-playbook vlan_provision.yml --tags \"add vlans\"\n under folder \"LTRDCN-1572\" as shown below:\n\n\n[root@rhel7-tools LTRDCN-1572]# ansible-playbook vlan_provision.yml --tags \"add vlans\"\n\n\n\n\n\nNote: If the playbook fails first time, re-run the playbook again. Make sure to save all the changes in the playbook first before executing the playbook in Ansible.\n\n\n\n\n\n\nAfter playbook is run successfully, login into leaf 3 again and check if vlan 140 and vlan 141 appears. There would also be a log message on the screen indicating a configuration change was pushed to the device:\n\n\n\n\n\n\n\n\n\n\nStep 6: Server port VLAN tasks in playbook\n\n\nWe have just tested our first playbook with basic configuration (i.e, by adding 2 VLANS), now we are going to add more tasks in our existing playbook \u201c\nvlan_provision.yml\n\u201d in this step:\n\n\n\n\nwe will add new plays in the playbook to assign VLANs to server facing port. This time, we will configure VLAN towards the server facing ports\n\n\nGo back to \nATOM\n and \nadd\n the following plays to the \nexisting playbook\n\n\n\n\n      - name: configure server facing port to L2\n        nxos_interface:\n          interface: eth1/3\n          mode: layer2\n          provider: \"{{nxos_provider}}\"\n      - name: configure VLAN for server port\n        when: (\"101\" in inventory_hostname) or (\"103\" in inventory_hostname)\n        nxos_switchport:\n          interface: eth1/3\n          mode: access\n          access_vlan: 140\n          provider: \"{{nxos_provider}}\"\n      - name: configure VLAN for server port\n        when: (\"102\" in inventory_hostname) or (\"104\" in inventory_hostname)\n        nxos_switchport:\n          interface: eth1/3\n          mode: access\n          access_vlan: 141\n          provider: \"{{nxos_provider}}\"\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n on ATOM. This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\nNote: In this new play, we used nxos module \u201c\nnxos_interface\n\u201d and \u201c\nnxos_switchport\n\u201d. \n\n\n\n\n\u201c\nnxos_interface\n\u201d provides the capability to manage the physical attributes of an interface\n\n\nIn this example, it is used to configure \u201clayer 2\u201d on interface Ethernet 1/3\n\n\n\n\n\n\n\u201c\nnxos_switchport\n\u201d provides the capability to manage the Layer 2 switchport attributes\n\n\nIn this example, it is used to configuration it is used to configure mode access on Ethernet ports 1/3\n\n\n\n\n\n\nWe used \u201c\nwhen\n\u201d argument to provide little logic of the play. \n\n\nIn our example, the playbook assign VLAN 140 on leaf1 and leaf3 switches; assign VLAN 141 on leaf4 switch. \n\n\n\n\n\n\n\n\n\n\nStep 7: Execute playbook\n\n\nNow, we are going to execute the playbook with the command \nansible-playbook vlan_provision.yml\n\n\n\n\n\n\nBefore excuting the ansible playbook, you may log into switch (leaf1, leaf3 or leaf4) using MTPutty client, and check the existing configuration by executing the below command:\n  \nshow run interface ethernet1/3\n\n\n\n\n\n\nOn MTPuttY, log back into (or launch a new ssh) into \u201cAnsible\u201d node\n\n\n\n\n\n\nExecute command \nansible-playbook vlan_provision.yml\n in the \u201cLTRDCN-1572\u201d directory as shown below:\n\n\n[root@rhel7-tools LTRDCN-1572]# ansible-playbook vlan_provision.yml\n\n\n\n\n\n\n\n\n\nAfter we push the configuration, \nlogin\n to the \nleaf-3\n switch, and check if the server facing port has the access vlan configured with the below command:\n  \nshow run interface ethernet1/3\n\n\nThe output of above command is shown in below screenshot:\n\n\n\n\n\n\n\n\n\n\nCongratulation! You have created your first ansible playbook, automatically provisioned new VLANs and assigned port to new created VLANs using Ansible. Next we are going to create VXLAN Fabric using Ansible.",
            "title": "Task 2 - First Simple Ansible Playbook"
        },
        {
            "location": "/task2-first-ansible/#step-1-using-atom-text-editor",
            "text": "Open \u201cAtom\u201d text editor by double click the icon on desktop. Atom is the recommended text editor for this lab:     After opening ATOM,  Click   No, Never  to the \u201cRegister as default atom:// URI handlre\u201d message as show below:",
            "title": "Step 1: Using \"Atom\" text Editor"
        },
        {
            "location": "/task2-first-ansible/#step-2-atom-folder",
            "text": "After opening ATOM, there should be a folder in the left pane named \u201cLTRDCN-1572.\u201d   Right click  the pre-configured project folder  LTRDCN-1572  and select  New File       Name the new file  vlan_provision.yml  and hit enter. This will create the new file:     Also, on the lower bar of the ATOM, verify that  file grammar of  YAML  is selected instead of default \" Plain Text \".  If  YAML  is not selected, then you should  choose  it from the listed options.",
            "title": "Step 2: Atom folder"
        },
        {
            "location": "/task2-first-ansible/#step-3-define-variables-tasks-for-playbook",
            "text": "In this step, we are going to define the scope, variable for playbook and tasks    In the opened window for \u2018vlan_provision.yml\u2019 file, enter the below content.  NOTE: YAML is space sensitive. Hence be careful with the spaces in below section.      ---                     \n#Task 2: Simple playbook assign VLAN to server facing port\n  - hosts: leaf:jinja2_leaf\n    vars:\n      nxos_provider:\n        username: \"{{ user }}\"\n        password: \"{{ pwd }}\"\n        transport: nxapi\n        host: \"{{ inventory_hostname }}\"  Note:   \u201c hosts: \u201d defines the scope of this playbook applies to all switches in group \u2018leaf\u2019 and \u2018jinja2_leaf\u2019 (within the  \"hosts\"  file created in pervious task).   Note  that you can review the IP addresses of the three (2) \u201cleaf\u201d and one (1) \u201cjinja2_leaf\u201d in the \u201c hosts \u201d file (configured in previous steps).  The IP addresses are:  jinja2_leaf: 198.18.4.104  leaf: 198.18.4.101  leaf: 198.18.4.103      \u201c vars \u201d defines one variable \u201c nxos_provider \u201d that will be used in this playbook.   \u201c nxos_provider \u201d is variable that includes all value that will be used for connection and autnentication.   This variable will be referenced in playbook via \u201c provider \u201d argument (that will be added in next step #4).",
            "title": "Step 3: Define variables, tasks for playbook"
        },
        {
            "location": "/task2-first-ansible/#step-4-vlan-tasks-in-playbook",
            "text": "Continue to add below tasks on the same  playbook file:       tasks:\n      - name: provision VLAN\n        nxos_config:\n          lines: \"vlan {{item}}\"\n          provider: \"{{nxos_provider}}\"\n        with_items:\n          -  140\n          -  141\n        tags: add vlans  Note:   Multiple plays can be defined in one playbook under \u201c tasks \u201d, each starts with \u201c-\u201c .    This kind of play creates multiple VLANs using  nxos_config  module.    The \u201clines\u201d option is used to pass only one configuration command.  This commands must be the exact same commands as found in the device running-config.  Though one or multiple configuration commands can be configured under the \u201clines\u201d option.   For multiple, ordered set of commands can be configured under this \u201cline\u201d section.     At the end of this play, we use \u201c tags \u201d to name the play \u201c add vlans \u201d. This is useful to run a specific part of the configuration without running whole playbook.    Below screenshot shows how playbook will look:   NOTE: Formatting is extremely important when working with Ansible. Ansible playbook would return errors if the spaces are not properly aligned or formatting is not correct    Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.    NOTE: Once the Save button is pressed, then at the lower part of ATOM app, you will see message about connecting to Ansibe host (198.18.134.150) and saving the vlan_provision.yml file",
            "title": "Step 4: VLAN tasks in playbook"
        },
        {
            "location": "/task2-first-ansible/#step-5-execute-playbook",
            "text": "After creating the playbook, it is now time to execute the playbook.    Before executing the playbook, we will verify the leaf switch if it has any vlan configuration present on it.   Login  to leaf-3 switch using the Mputty client, or any leaf switch in previous playbook (remember  hosts:  variable in the file), and execute  show vlan brief  command.    This command will show the vlans that currently exist on the leaf switch.  As you note from below screenshot, only the default VLAN (vlan number 1) is configured:      Now, go to mputty, login or launch a new ssh into Ansible node (198.18.134.150)     Use command  ansible-playbook vlan_provision.yml --tags \"add vlans\"  under folder \"LTRDCN-1572\" as shown below:  [root@rhel7-tools LTRDCN-1572]# ansible-playbook vlan_provision.yml --tags \"add vlans\"   Note: If the playbook fails first time, re-run the playbook again. Make sure to save all the changes in the playbook first before executing the playbook in Ansible.    After playbook is run successfully, login into leaf 3 again and check if vlan 140 and vlan 141 appears. There would also be a log message on the screen indicating a configuration change was pushed to the device:",
            "title": "Step 5: Execute playbook"
        },
        {
            "location": "/task2-first-ansible/#step-6-server-port-vlan-tasks-in-playbook",
            "text": "We have just tested our first playbook with basic configuration (i.e, by adding 2 VLANS), now we are going to add more tasks in our existing playbook \u201c vlan_provision.yml \u201d in this step:   we will add new plays in the playbook to assign VLANs to server facing port. This time, we will configure VLAN towards the server facing ports  Go back to  ATOM  and  add  the following plays to the  existing playbook         - name: configure server facing port to L2\n        nxos_interface:\n          interface: eth1/3\n          mode: layer2\n          provider: \"{{nxos_provider}}\"\n      - name: configure VLAN for server port\n        when: (\"101\" in inventory_hostname) or (\"103\" in inventory_hostname)\n        nxos_switchport:\n          interface: eth1/3\n          mode: access\n          access_vlan: 140\n          provider: \"{{nxos_provider}}\"\n      - name: configure VLAN for server port\n        when: (\"102\" in inventory_hostname) or (\"104\" in inventory_hostname)\n        nxos_switchport:\n          interface: eth1/3\n          mode: access\n          access_vlan: 141\n          provider: \"{{nxos_provider}}\"   Click   File  and  Save  on ATOM. This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.    Note: In this new play, we used nxos module \u201c nxos_interface \u201d and \u201c nxos_switchport \u201d.    \u201c nxos_interface \u201d provides the capability to manage the physical attributes of an interface  In this example, it is used to configure \u201clayer 2\u201d on interface Ethernet 1/3    \u201c nxos_switchport \u201d provides the capability to manage the Layer 2 switchport attributes  In this example, it is used to configuration it is used to configure mode access on Ethernet ports 1/3    We used \u201c when \u201d argument to provide little logic of the play.   In our example, the playbook assign VLAN 140 on leaf1 and leaf3 switches; assign VLAN 141 on leaf4 switch.",
            "title": "Step 6: Server port VLAN tasks in playbook"
        },
        {
            "location": "/task2-first-ansible/#step-7-execute-playbook",
            "text": "Now, we are going to execute the playbook with the command  ansible-playbook vlan_provision.yml    Before excuting the ansible playbook, you may log into switch (leaf1, leaf3 or leaf4) using MTPutty client, and check the existing configuration by executing the below command:\n   show run interface ethernet1/3    On MTPuttY, log back into (or launch a new ssh) into \u201cAnsible\u201d node    Execute command  ansible-playbook vlan_provision.yml  in the \u201cLTRDCN-1572\u201d directory as shown below:  [root@rhel7-tools LTRDCN-1572]# ansible-playbook vlan_provision.yml     After we push the configuration,  login  to the  leaf-3  switch, and check if the server facing port has the access vlan configured with the below command:\n   show run interface ethernet1/3  The output of above command is shown in below screenshot:      Congratulation! You have created your first ansible playbook, automatically provisioned new VLANs and assigned port to new created VLANs using Ansible. Next we are going to create VXLAN Fabric using Ansible.",
            "title": "Step 7: Execute playbook"
        },
        {
            "location": "/task3-vxlan-jinja2/",
            "text": "In this task, we are going to install Jinja2. It is one of the python template engines.  In this section, we use Jinja2 to create template for spine and leaf and configure VXLAN fabric using this Jinja2 templates. \n\n\n\n\nJinja2 template looks just like the NXOS configurations. We abstract the variables out of the configuration and use simple for loop to feed variables into the template. \n\n\n\n\nStep 1: Install jinja2\n\n\n\n\n\n\nOn the Ansible node, install jinja2 using \npip install jinja2\n command.  If it is already installed, we will get the message \u201crequirement is satisfied\u201d:\n\n\n[root@rhel7-tools ~]# pip install jinja2\nRequirement already satisfied (use --upgrade to upgrade): jinja2 in /usr/lib/python2.7/site-packages/Jinja2-2.8-py2.7.egg\nRequirement already satisfied (use --upgrade to upgrade): MarkupSafe in /usr/lib/python2.7/site-packages/MarkupSafe-0.23-py2.7.egg (from jinja2)\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Playbook for jinja2 Spine\n\n\nIn this section, we will use Jina2 template and Ansible to provision the VXLAN Fabric. \n\n\n\n\n\n\nSwitch to \u201cAtom\u201d, \nright click\n on the folder 'LTRDDCN-1572' and create a new playbook named \njinja2_fabric.yml\n. Enter this file name and hit enter\n\n\n\n\n\n\n\n\nAlso, on the lower bar of the ATOM, verify that  file grammar of \nYAML\n is selected instead of default \"\nPlain Text\n\".  If \nYAML\n is not selected, then you should \nchoose\n it from the listed options.\n\n\n\n\n\n\nNow enter below data in this playbook:\n\n\n\n\n\n\n---\n  - hosts: jinja2_spine\n    connection: local\n    roles:\n    - jinja2_spine\n\n\n\n\n\n\n\n\n\nThe contents of the \njinja2_fabric.yml\n file should look like\n\n\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\n\n\nOn the MTputty, go back to the ssh session to Ansible Server node (198.18.134.150). \nVerify\n that the below 2 groups exists in the inventory filename \"\nhosts\n\" under the folder LTRDCN-1572\n\n\n[jinja2_spine]\n198.18.4.202 router_id=192.168.0.7 loopback1=192.168.0.100\n[jinja2_leaf]\n198.18.4.104 router_id=192.168.0.11 loopback1=192.168.0.111\n\n\n\n\n\n\n\n\n\nStep 3: Create new roles and vars\n\n\nIn this section, we will create two new roles for provisioning Fabric with Jina2 template.\n\n\n\n\n\n\nOn the MTputty, go back to Ansible Server node (198.18.134.150), switch to \u2018\nroles\n\u2019 directory; \ncreate\n \u2018jinja2_spine\u2019 and \u2018jinja2_leaf\u2019 roles using ansible-galaxy using below commands: \n\n\n[root@rhel7-tools ~]# ~/cd LTRDCN-1572/\n[root@rhel7-tools LTRDCN-1572]# cd roles/\n[root@rhel7-tools roles]# ansible-galaxy init jinja2_spine&&ansible-galaxy init jinja2_leaf\n\n\n\n\n\n\n\nBelow screenshot shows the output of above command:\n\n\n\n\n\n\n\n\nNote:\n \u2018\nansible-galaxy\n\u2019 will initialize the role structure and create necessary folders with default name like \u2018tasks\u2019, \u2018template\u2019, \u2018vars\u2019 etc.\n\n\n\n\n\n\nchange directory\n path to LTRDCN-1572/roles/jinja2_spine and check the content of local directory (\nls\n) as per below commands:\n\n\n[root@rhel7-tools]# cd ~/LTRDCN-1572/roles/jinja2_spine/\n[root@rhel7-tools jinja2_spine]# ls\n\n\n\n\n\n\n\nBelow screenshot shows the output of above file.  Note that various directories including tasks, templates, vars exists.  We will use these in later steps. \n\n\n\n\n\n\n\n\nNext:\n\n\n\n\n\n\nCreate\n empty jinja2 template files for spine and leaf under templates folder for each role by running below commands:\n\n\n[root@rhel7-tools roles]# cd ~/LTRDCN-1572/roles\n[root@rhel7-tools roles]# touch jinja2_spine/templates/spine.j2\n[root@rhel7-tools roles]# touch jinja2_leaf/templates/leaf.j2\n\n\n\n\n\n\n\nSwitch to \u201c\nAtom\n\u201d and sync the new created folders between Ansible node and Remote desktop by pressing \nRight Click\n on the folder \nLTRDDCN-1572\n, then open \nRemote Sync\n select \nDownload Folder\n as shown below:\n\n\n\n\n\n\n\n\n\n\nStep 4: Create variable file for \u201cjina2_spine\u201d role\n\n\n\u201c\nansible-galaxy\n\u201d automatically creates empty \u201c\nmain.yml\n\u201d file under \u201c\nvars\n\u201d folder. We can use \u201c\nAtom\n\u201d to edit the main.yml file to include the following variables that will be used in jinja2 template. \n\n\n\n\n\n\nSwitch to \nATOM\n, then open up the project folder \nLTRDCN-1572\n from the left pane and \nopen\n \nmain.yml\n file under \u201c\nroles/jinja2_spine/vars/\n\u201d as shown below:\n\n\n\n\n\n\n\n\nuse \u201c\nAtom\n\u201d to edit the \u201c\nmain.yml\n\u201d file to include the following variables that will be used in jinja2 template. \n\n\n\n\n\n\n---\n# vars file for jinja2_spine\n  nxos_provider:\n    username: \"{{ user }}\"\n    password: \"{{ pwd }}\"\n    timeout: 100\n    host: \"{{ inventory_hostname }}\"\n  asn: 65000\n  bgp_neighbors:\n  -  remote_as: 65000\n     neighbor: 192.168.0.8\n     update_source: Loopback0\n  -  remote_as: 65000\n     neighbor: 192.168.0.10\n     update_source: Loopback0\n  -  remote_as: 65000\n     neighbor: 192.168.0.11\n     update_source: Loopback0\n  L3_interfaces:\n  -  interface: Ethernet 1/1\n  -  interface: Ethernet 1/2\n  -  interface: Ethernet 1/3\n  -  interface: Ethernet 1/4\n  -  interface: loopback 0\n  -  interface: loopback 1\n  s1_loopback: 192.168.0.6\n  s2_loopback: 192.168.0.7\n\n\n\n\n\n\n\n\nContents of the \u2018\nmain.yml\n\u2019 file should look like below:\n\n\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\n\n\n\n\nStep 5: Create Jinja2 template for spine role\n\n\n\n\nOn \nATOM\n open up the project folder \nLTRDCN-1572\n from the left pane and \nopen\n \nspine.j2\n file under \u201c\nroles/jinja2_spine/templates\n\u201d as shown below:\n\n\n\n\nNOTE: if the file does not appear on the ATOM, then go ahead and execute below 4 steps to get it sync.  If the \nspine.j2\n file appears in above folder then you can skip below 4 steps\n:\n\n\n\n\nChange Directory to folder LTRDCN-1572 on Ansible server (198.18.134.150) using \ncd ~\\ LTRDC-1572\n command\n\n\nfurther, change Directory (cd) to folder roles/jinja2_spine/templates using \ncd roles/jinja2_spine/templates\n command\n\n\nType \ntouch spine.j2\n\n\n\n\nAfter entering the command, go back to ATOM,  \nright click\n on folder \nLTRDCN-1572\n, scroll to choose option \nRemote Sync\n option and choose \nDownload Folder\n as shown below: \n\n\n\n\n\n\n\n\nNow that the file/folder appears properly on ATOM, go ahead and proceed with further steps:\n\n\n\n\n\n\nTo reduce the typo, you can download jina2 template from the \nbox\n folder spine.j2. Below is the link to this folder:\n\n\nhttps://cisco.app.box.com/v/LTRDCN1572\n\n\n\n\n\n\nThe file would be under LTRDCN-1572/roles/jinja2_spine/templates/spine.j2 (link below) as shown in below screenshot:\n\n\nhttps://cisco.app.box.com/v/LTRDCN1572/folder/44981931157\n\n\n\n\n\n\n\n\n\nAfter the file is downloaded to the PC (on your RDP session), go to the downloads folder. Move (or copy) the file from the downloads folder and paste the file in the projects folder\n\n\nTFTP_Data (\\\\AD1) (X:)\n -->  \nLTRDC-1572\n --> \nroles\n --> \njinja2_spine\n --> \ntemplates\n\n\nas shown below:\n\n\n\n\n\n\n\n\nOn ATOM, go to \nFile\n then \nOpen File\u2026\n and browse to this \nspine.j2\n file that was just saved in \nX:\\LTRDCN-1572\\roles\\jinja2_spine\\templates\n as shown below:\n    \n\n\n\n\n\n\nAfter opening \u201c\nspine.j2\n\u201d file from ATOM, go to \nFile\n \u2013-> \nSave\n to push template file to Ansible node:\n\n\n\n\n\n\n\n\n\n\nStep 6: Create playbook for jinja2_spine role\n\n\nThe playbook for jinja2_spine roles has two tasks. First task uses ansible \ntemplate\n module to generate configuration file based on jinja2 template created in last step. The configuration file is saved in \u201c\nfile\n\u201d folder. Second task is push the configuration to switch. \n\n\n\u201c\nansible-galaxy\n\u201d automatically creates empty \u201c\nmain.yml\n\u201d file under \u201c\ntasks\n\u201d folder.  We are going to use \u201c\nAtom\n\u201d to edit the \nmain.yml\n file. \n\n\n\n\nOn ATOM, open up the project folder \nLTRDCN-1572\n and edit \nmain.yml\n file under \nroles/jinja2_spine/tasks/\n to include following: \n\n\n\n\n---\n# tasks file for jinja2_spine\n  - name: Generate Spine Config\n    template: src=spine.j2 dest=roles/jinja2_spine/files/{{inventory_hostname}}.cfg\n  - name: Push Spine Config\n    ios_config:\n      src: roles/jinja2_spine/files/{{inventory_hostname}}.cfg\n      force: yes\n      provider: \"{{ nxos_provider }}\"\n\n\n\n\n\n\n\n\nContents of the \u2018\nmain.yml\n\u2019 file should look like below:\n\n\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\n\n\nNOTE:\n In the above YAML file, ansible module named \u201c\nios_config\n\u201d is used.  This module performs below activities:\n\n\n\n\nIt uses source path of the file (\u201c\nsrc\n\u201d) that contains the configuration or configuration template to load into spine\n\n\nSince \u201c\nforce\n\u201d option is set to true (yes), hence the contents of src file will be pushed into device without checking if these already exist in device\n\n\n\n\n\n\nStep 7: Run Jinja2_fabric playbook\n\n\nIn this section you will run the playbook created in step 2 (in this task 3), this will generate configuration file for Spine-2 switche from the template. \n\n\nThe playbook will also push the configuration file to Spine-2 switches. \n\n\n\n\n\n\nRun the ansible playbook by going to folder LTRDC-1572 and executing the below commands:\n\n\n[root@rhel7-tools roles]# cd ~/LTRDCN-1572/\n[root@rhel7-tools LTRDCN-1572]# ansible-playbook jinja2_fabric.yml\n\n\n\nNote: It will take few minutes to push configuration\n\n\nBelow screenshot shows the execution of above playbook:\n\n\n\n\n\n\n\n\nTo verify the execution of this playbook, you can:\n\n\n\n\nLogin\n to \nSpine-2 \nswitch (\non MTputty\n) to verify configuration has been pushed. Double click the spine-2 icon in the left pane on MTputty.  Login with credentials admin/C1sco12345\n\n\n\n\nExecute \nshow run bgp\n command on the switch to confirm the configurations have been provisioned (as shown below):\n\n\n\n\n\n\n\n\n\n\nStep 8: Modify playbook for Leaf\n\n\nIn this section, we will use Jina2 template and Ansible to provision the VXLAN Fabric on leaf-4.   We are going to add jinja2_leaf this time to the already created playbook in step 2.\n\n\n\n\nSwitch to \u201c\nAtom\n\u201d, click on the folder \nLTRDDCN-1572\n, edit the existing playbook \njinja2_fabric.yml\n file: \n\n\nBelow screenshot shows the contents of \njinja2_fabric.yml\n file in Atom:\n\n\n\n\n\n\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\n\n\nStep 9: Variable file for jinja2_leaf role\n\n\n\n\nOn ATOM, open up the project folder \nLTRDCN-1572\n and edit \nmain.yml\n file under \nroles/jinja2_leaf/vars/\n to include following:\n\n\n\n\n---\n# vars file for jinja2_leaf\n  nxos_provider:\n    username: \"{{ user }}\"\n    password: \"{{ pwd }}\"\n    timeout: 100\n    host: \"{{ inventory_hostname }}\"\n  asn: 65000\n  bgp_neighbors:\n  -  remote_as: 65000\n     neighbor: 192.168.0.6\n     update_source: Loopback0\n  -  remote_as: 65000\n     neighbor: 192.168.0.7\n     update_source: Loopback0\n  rp_address: 192.168.0.100\n  L3_interfaces:\n  -  interface: Ethernet 1/1\n  -  interface: Ethernet 1/2\n  -  interface: loopback 0\n  -  interface: loopback 1\n  L2VNI:\n  -  vlan_id: 140\n     vni: 50140\n     ip_add: 172.21.140.1\n     mask: 24\n     vlan_name: L2-VNI-140-Tenant1\n     mcast: 239.0.0.140\n  -  vlan_id: 141\n     vni: 50141\n     ip_add: 172.21.141.1\n     mask: 24\n     vlan_name: L2-VNI-141-Tenant1\n     mcast: 239.0.0.141\n  L3VNI:\n  -  vlan_id: 999\n     vlan_name: L3-VNI-999-Tenant1\n     vni: 50999\n\n\n\n\n\n\n\n\nBelow screenshot shows the contents of jinja2_leaf\\vars\\main.yml file in Atom:\n\n\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\n\n\n\n\nStep 10: Jinja2 template for leaf role\n\n\n\n\nOn ATOM, open up the project folder \nLTRDCN-1572\n and open \nleaf.j2\n file under \u201c\nroles/jinja2_leaf/templates/\n\u201d\n\n\n\n\nNOTE: if the file does not appear on the ATOM, then go ahead and execute below 4 steps to get it sync.  If the \nleaf.j2\n file appears in above folder then you can skip below 4 steps\n:\n\n\n\n\nOn MTputty, change Directory to folder LTRDCN-1572 on Ansible server (198.18.134.150) using \ncd ~\\ LTRDC-1572\n command\n\n\nfurther, change Directory (cd) to folder roles/jinja2_leaf/templates using \ncd roles/jinja2_leaf/templates\n command\n\n\nType \ntouch leaf.j2\n\n\nAfter entering the command, go back to ATOM,  \nright click\n on folder \nLTRDCN-1572\n, scroll to choose option \nRemote Sync\n option and choose \nDownload Folder\n\n\n\n\nNow that the file/folder appears properly on ATOM, go ahead and proceed with further steps:\n\n\n\n\n\n\nTo reduce the typo, you can download jina2 template from the \nbox\n folder leaf.j2. Below is the link to this folder:\n\n\n\n\n\n\nTo reduce the typo, you can download jina2 template from the \nbox\n folder leaf.j2. Below is the link to this folder:\n\n\nhttps://cisco.app.box.com/v/LTRDCN1572\n\n\n\n\n\n\nThe file would be under LTRDCN-1572/roles/jinja2_leaf/templates/leaf.j2\n(link below) as shown in below screenshot:\n\n\nhttps://cisco.app.box.com/v/LTRDCN1572/folder/44981518497\n\n\n\n\n\n\n\n\n\nAfter the file is downloaded to the PC (on your RDP session), go to the downloads folder. Move (or copy) the file from the downloads folder and paste the file in the projects folder\n\n\nTFTP_Data (\\\\AD1) (X:)\n -->  \nLTRDC-1572\n --> \nroles\n --> \njinja2_leaf\n --> \ntemplates\n\n\nas shown below:\n\n\n\n\n\n\n\n\nOn ATOM, go to \nFile\n then \nOpen File\u2026\n and browse to this \nleaf.j2\n that was just saved in \nX:\\LTRDCN-1572\\roles\\jinja2_leaf\\templates\n as shown below screenshots:\n    \n\n\n\n\n\n\n\n\nAfter opening \u201c\nleaf.j2\n\u201d file from ATOM, go to \nFile\n \u2013 \nSave\n to push template file to Ansible node:\n\n\n\n\n\n\n\n\nStep 11: Create playbook for jinja2_leaf role\n\n\nThe playbook for jinja2_leaf roles has two tasks. \n\n\n\n\nFirst task uses ansible template module to generate configuration file based on jinja2 template created in last step. The configuration file is saved in \u201cfile\u201d folder. \n\n\nSecond task is to push the configuration to switch. \n\n\n\n\n\u201c\nansible-galaxy\n\u201d automatically creates empty \u201c\nmain.yml\n\u201d file under \u201c\ntasks\n\u201d folder.  We are going to use \u201c\nAtom\n\u201d to edit this main.yml file. \n\n\n\n\nOn ATOM, open up the project folder \nLTRDCN-1572\n and edit \u201c\nmain.yml\n\u201d file under \u201c\nroles/jinja2_leaf/tasks/\n\u201d to include following:\n\n\n\n\n---\n# tasks file for jinja2_leaf\n  - name: Generate Leaf Config\n    template: src=leaf.j2 dest=roles/jinja2_leaf/files/{{inventory_hostname}}.cfg\n  - name: Push Leaf Config\n    ios_config:\n      src: roles/jinja2_leaf/files/{{inventory_hostname}}.cfg\n      force: yes\n      provider: \"{{ nxos_provider }}\"\n\n\n\n\nBelow screenshot shows how the contents of \njinja2_leaf/taks/main.yml\n file looks like in Atom:\n\n\n\n\n\n\nStep 12: Run Jinja2_fabric playbook\n\n\nIn this section you will run the playbook created in step 8, this will generate configuration file for Spine-2 and Leaf-4 switches. It will also push the configuration file to both switches. \n\n\n\n\n\n\nBefore running the ansible-playbook, you may \nlog\n into the leaf-4 (in MTputty SSH session) and verify that no bgp configurations exist by running \nshow running bgp\n command as shown below:\n\n\n\n\n\n\n\n\nNOTE: It might take couple of minutes for the configuration to be pushed to via the Ansible Server. It is working in the background. \n\n\n\n\n\n\nOn the Ansible node (in MTputty SSH session), run the command (\nansible-playbook jinja2_fabric.yml\n) to execute the playbook as shown below:\n\n\n[root@rhel7-tools LTRDCN-1572]# ansible-playbook jinja2_fabric.yml\n\n\n\nBelow screenshot shows the output of above command:\n\n\n\n\n\n\n\n\nAfter the configuration push is successful, \nlogin\n (on MTputty SSH session) to \nleaf-4\n switch to verify configuration has been pushed by running below command:\n\n\nshow running-config bgp\n\n\nthe output of above command is shown below:\n\n\n\n\n\n\n\n\n\n\nCongrats: you have successfully concluded this task by using jinja2 templates with Ansible for Cisco Nexus switches",
            "title": "Task 3 - Use of Jinja2 templates with Ansible Playbook"
        },
        {
            "location": "/task3-vxlan-jinja2/#step-1-install-jinja2",
            "text": "On the Ansible node, install jinja2 using  pip install jinja2  command.  If it is already installed, we will get the message \u201crequirement is satisfied\u201d:  [root@rhel7-tools ~]# pip install jinja2\nRequirement already satisfied (use --upgrade to upgrade): jinja2 in /usr/lib/python2.7/site-packages/Jinja2-2.8-py2.7.egg\nRequirement already satisfied (use --upgrade to upgrade): MarkupSafe in /usr/lib/python2.7/site-packages/MarkupSafe-0.23-py2.7.egg (from jinja2)",
            "title": "Step 1: Install jinja2"
        },
        {
            "location": "/task3-vxlan-jinja2/#step-2-playbook-for-jinja2-spine",
            "text": "In this section, we will use Jina2 template and Ansible to provision the VXLAN Fabric.     Switch to \u201cAtom\u201d,  right click  on the folder 'LTRDDCN-1572' and create a new playbook named  jinja2_fabric.yml . Enter this file name and hit enter     Also, on the lower bar of the ATOM, verify that  file grammar of  YAML  is selected instead of default \" Plain Text \".  If  YAML  is not selected, then you should  choose  it from the listed options.    Now enter below data in this playbook:    ---\n  - hosts: jinja2_spine\n    connection: local\n    roles:\n    - jinja2_spine    The contents of the  jinja2_fabric.yml  file should look like     Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.     On the MTputty, go back to the ssh session to Ansible Server node (198.18.134.150).  Verify  that the below 2 groups exists in the inventory filename \" hosts \" under the folder LTRDCN-1572  [jinja2_spine]\n198.18.4.202 router_id=192.168.0.7 loopback1=192.168.0.100\n[jinja2_leaf]\n198.18.4.104 router_id=192.168.0.11 loopback1=192.168.0.111",
            "title": "Step 2: Playbook for jinja2 Spine"
        },
        {
            "location": "/task3-vxlan-jinja2/#step-3-create-new-roles-and-vars",
            "text": "In this section, we will create two new roles for provisioning Fabric with Jina2 template.    On the MTputty, go back to Ansible Server node (198.18.134.150), switch to \u2018 roles \u2019 directory;  create  \u2018jinja2_spine\u2019 and \u2018jinja2_leaf\u2019 roles using ansible-galaxy using below commands:   [root@rhel7-tools ~]# ~/cd LTRDCN-1572/\n[root@rhel7-tools LTRDCN-1572]# cd roles/\n[root@rhel7-tools roles]# ansible-galaxy init jinja2_spine&&ansible-galaxy init jinja2_leaf    Below screenshot shows the output of above command:     Note:  \u2018 ansible-galaxy \u2019 will initialize the role structure and create necessary folders with default name like \u2018tasks\u2019, \u2018template\u2019, \u2018vars\u2019 etc.    change directory  path to LTRDCN-1572/roles/jinja2_spine and check the content of local directory ( ls ) as per below commands:  [root@rhel7-tools]# cd ~/LTRDCN-1572/roles/jinja2_spine/\n[root@rhel7-tools jinja2_spine]# ls    Below screenshot shows the output of above file.  Note that various directories including tasks, templates, vars exists.  We will use these in later steps.      Next:    Create  empty jinja2 template files for spine and leaf under templates folder for each role by running below commands:  [root@rhel7-tools roles]# cd ~/LTRDCN-1572/roles\n[root@rhel7-tools roles]# touch jinja2_spine/templates/spine.j2\n[root@rhel7-tools roles]# touch jinja2_leaf/templates/leaf.j2    Switch to \u201c Atom \u201d and sync the new created folders between Ansible node and Remote desktop by pressing  Right Click  on the folder  LTRDDCN-1572 , then open  Remote Sync  select  Download Folder  as shown below:",
            "title": "Step 3: Create new roles and vars"
        },
        {
            "location": "/task3-vxlan-jinja2/#step-4-create-variable-file-for-jina2_spine-role",
            "text": "\u201c ansible-galaxy \u201d automatically creates empty \u201c main.yml \u201d file under \u201c vars \u201d folder. We can use \u201c Atom \u201d to edit the main.yml file to include the following variables that will be used in jinja2 template.     Switch to  ATOM , then open up the project folder  LTRDCN-1572  from the left pane and  open   main.yml  file under \u201c roles/jinja2_spine/vars/ \u201d as shown below:     use \u201c Atom \u201d to edit the \u201c main.yml \u201d file to include the following variables that will be used in jinja2 template.     ---\n# vars file for jinja2_spine\n  nxos_provider:\n    username: \"{{ user }}\"\n    password: \"{{ pwd }}\"\n    timeout: 100\n    host: \"{{ inventory_hostname }}\"\n  asn: 65000\n  bgp_neighbors:\n  -  remote_as: 65000\n     neighbor: 192.168.0.8\n     update_source: Loopback0\n  -  remote_as: 65000\n     neighbor: 192.168.0.10\n     update_source: Loopback0\n  -  remote_as: 65000\n     neighbor: 192.168.0.11\n     update_source: Loopback0\n  L3_interfaces:\n  -  interface: Ethernet 1/1\n  -  interface: Ethernet 1/2\n  -  interface: Ethernet 1/3\n  -  interface: Ethernet 1/4\n  -  interface: loopback 0\n  -  interface: loopback 1\n  s1_loopback: 192.168.0.6\n  s2_loopback: 192.168.0.7    Contents of the \u2018 main.yml \u2019 file should look like below:     Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.",
            "title": "Step 4: Create variable file for \u201cjina2_spine\u201d role"
        },
        {
            "location": "/task3-vxlan-jinja2/#step-5-create-jinja2-template-for-spine-role",
            "text": "On  ATOM  open up the project folder  LTRDCN-1572  from the left pane and  open   spine.j2  file under \u201c roles/jinja2_spine/templates \u201d as shown below:   NOTE: if the file does not appear on the ATOM, then go ahead and execute below 4 steps to get it sync.  If the  spine.j2  file appears in above folder then you can skip below 4 steps :   Change Directory to folder LTRDCN-1572 on Ansible server (198.18.134.150) using  cd ~\\ LTRDC-1572  command  further, change Directory (cd) to folder roles/jinja2_spine/templates using  cd roles/jinja2_spine/templates  command  Type  touch spine.j2   After entering the command, go back to ATOM,   right click  on folder  LTRDCN-1572 , scroll to choose option  Remote Sync  option and choose  Download Folder  as shown below:      Now that the file/folder appears properly on ATOM, go ahead and proceed with further steps:    To reduce the typo, you can download jina2 template from the  box  folder spine.j2. Below is the link to this folder:  https://cisco.app.box.com/v/LTRDCN1572    The file would be under LTRDCN-1572/roles/jinja2_spine/templates/spine.j2 (link below) as shown in below screenshot:  https://cisco.app.box.com/v/LTRDCN1572/folder/44981931157     After the file is downloaded to the PC (on your RDP session), go to the downloads folder. Move (or copy) the file from the downloads folder and paste the file in the projects folder  TFTP_Data (\\\\AD1) (X:)  -->   LTRDC-1572  -->  roles  -->  jinja2_spine  -->  templates  as shown below:     On ATOM, go to  File  then  Open File\u2026  and browse to this  spine.j2  file that was just saved in  X:\\LTRDCN-1572\\roles\\jinja2_spine\\templates  as shown below:\n        After opening \u201c spine.j2 \u201d file from ATOM, go to  File  \u2013->  Save  to push template file to Ansible node:",
            "title": "Step 5: Create Jinja2 template for spine role"
        },
        {
            "location": "/task3-vxlan-jinja2/#step-6-create-playbook-for-jinja2_spine-role",
            "text": "The playbook for jinja2_spine roles has two tasks. First task uses ansible  template  module to generate configuration file based on jinja2 template created in last step. The configuration file is saved in \u201c file \u201d folder. Second task is push the configuration to switch.   \u201c ansible-galaxy \u201d automatically creates empty \u201c main.yml \u201d file under \u201c tasks \u201d folder.  We are going to use \u201c Atom \u201d to edit the  main.yml  file.    On ATOM, open up the project folder  LTRDCN-1572  and edit  main.yml  file under  roles/jinja2_spine/tasks/  to include following:    ---\n# tasks file for jinja2_spine\n  - name: Generate Spine Config\n    template: src=spine.j2 dest=roles/jinja2_spine/files/{{inventory_hostname}}.cfg\n  - name: Push Spine Config\n    ios_config:\n      src: roles/jinja2_spine/files/{{inventory_hostname}}.cfg\n      force: yes\n      provider: \"{{ nxos_provider }}\"    Contents of the \u2018 main.yml \u2019 file should look like below:     Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.     NOTE:  In the above YAML file, ansible module named \u201c ios_config \u201d is used.  This module performs below activities:   It uses source path of the file (\u201c src \u201d) that contains the configuration or configuration template to load into spine  Since \u201c force \u201d option is set to true (yes), hence the contents of src file will be pushed into device without checking if these already exist in device",
            "title": "Step 6: Create playbook for jinja2_spine role"
        },
        {
            "location": "/task3-vxlan-jinja2/#step-7-run-jinja2_fabric-playbook",
            "text": "In this section you will run the playbook created in step 2 (in this task 3), this will generate configuration file for Spine-2 switche from the template.   The playbook will also push the configuration file to Spine-2 switches.     Run the ansible playbook by going to folder LTRDC-1572 and executing the below commands:  [root@rhel7-tools roles]# cd ~/LTRDCN-1572/\n[root@rhel7-tools LTRDCN-1572]# ansible-playbook jinja2_fabric.yml  Note: It will take few minutes to push configuration  Below screenshot shows the execution of above playbook:     To verify the execution of this playbook, you can:   Login  to  Spine-2  switch ( on MTputty ) to verify configuration has been pushed. Double click the spine-2 icon in the left pane on MTputty.  Login with credentials admin/C1sco12345   Execute  show run bgp  command on the switch to confirm the configurations have been provisioned (as shown below):",
            "title": "Step 7: Run Jinja2_fabric playbook"
        },
        {
            "location": "/task3-vxlan-jinja2/#step-8-modify-playbook-for-leaf",
            "text": "In this section, we will use Jina2 template and Ansible to provision the VXLAN Fabric on leaf-4.   We are going to add jinja2_leaf this time to the already created playbook in step 2.   Switch to \u201c Atom \u201d, click on the folder  LTRDDCN-1572 , edit the existing playbook  jinja2_fabric.yml  file:   Below screenshot shows the contents of  jinja2_fabric.yml  file in Atom:       Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.",
            "title": "Step 8: Modify playbook for Leaf"
        },
        {
            "location": "/task3-vxlan-jinja2/#step-9-variable-file-for-jinja2_leaf-role",
            "text": "On ATOM, open up the project folder  LTRDCN-1572  and edit  main.yml  file under  roles/jinja2_leaf/vars/  to include following:   ---\n# vars file for jinja2_leaf\n  nxos_provider:\n    username: \"{{ user }}\"\n    password: \"{{ pwd }}\"\n    timeout: 100\n    host: \"{{ inventory_hostname }}\"\n  asn: 65000\n  bgp_neighbors:\n  -  remote_as: 65000\n     neighbor: 192.168.0.6\n     update_source: Loopback0\n  -  remote_as: 65000\n     neighbor: 192.168.0.7\n     update_source: Loopback0\n  rp_address: 192.168.0.100\n  L3_interfaces:\n  -  interface: Ethernet 1/1\n  -  interface: Ethernet 1/2\n  -  interface: loopback 0\n  -  interface: loopback 1\n  L2VNI:\n  -  vlan_id: 140\n     vni: 50140\n     ip_add: 172.21.140.1\n     mask: 24\n     vlan_name: L2-VNI-140-Tenant1\n     mcast: 239.0.0.140\n  -  vlan_id: 141\n     vni: 50141\n     ip_add: 172.21.141.1\n     mask: 24\n     vlan_name: L2-VNI-141-Tenant1\n     mcast: 239.0.0.141\n  L3VNI:\n  -  vlan_id: 999\n     vlan_name: L3-VNI-999-Tenant1\n     vni: 50999    Below screenshot shows the contents of jinja2_leaf\\vars\\main.yml file in Atom:     Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.",
            "title": "Step 9: Variable file for jinja2_leaf role"
        },
        {
            "location": "/task3-vxlan-jinja2/#step-10-jinja2-template-for-leaf-role",
            "text": "On ATOM, open up the project folder  LTRDCN-1572  and open  leaf.j2  file under \u201c roles/jinja2_leaf/templates/ \u201d   NOTE: if the file does not appear on the ATOM, then go ahead and execute below 4 steps to get it sync.  If the  leaf.j2  file appears in above folder then you can skip below 4 steps :   On MTputty, change Directory to folder LTRDCN-1572 on Ansible server (198.18.134.150) using  cd ~\\ LTRDC-1572  command  further, change Directory (cd) to folder roles/jinja2_leaf/templates using  cd roles/jinja2_leaf/templates  command  Type  touch leaf.j2  After entering the command, go back to ATOM,   right click  on folder  LTRDCN-1572 , scroll to choose option  Remote Sync  option and choose  Download Folder   Now that the file/folder appears properly on ATOM, go ahead and proceed with further steps:    To reduce the typo, you can download jina2 template from the  box  folder leaf.j2. Below is the link to this folder:    To reduce the typo, you can download jina2 template from the  box  folder leaf.j2. Below is the link to this folder:  https://cisco.app.box.com/v/LTRDCN1572    The file would be under LTRDCN-1572/roles/jinja2_leaf/templates/leaf.j2\n(link below) as shown in below screenshot:  https://cisco.app.box.com/v/LTRDCN1572/folder/44981518497     After the file is downloaded to the PC (on your RDP session), go to the downloads folder. Move (or copy) the file from the downloads folder and paste the file in the projects folder  TFTP_Data (\\\\AD1) (X:)  -->   LTRDC-1572  -->  roles  -->  jinja2_leaf  -->  templates  as shown below:     On ATOM, go to  File  then  Open File\u2026  and browse to this  leaf.j2  that was just saved in  X:\\LTRDCN-1572\\roles\\jinja2_leaf\\templates  as shown below screenshots:\n         After opening \u201c leaf.j2 \u201d file from ATOM, go to  File  \u2013  Save  to push template file to Ansible node:",
            "title": "Step 10: Jinja2 template for leaf role"
        },
        {
            "location": "/task3-vxlan-jinja2/#step-11-create-playbook-for-jinja2_leaf-role",
            "text": "The playbook for jinja2_leaf roles has two tasks.    First task uses ansible template module to generate configuration file based on jinja2 template created in last step. The configuration file is saved in \u201cfile\u201d folder.   Second task is to push the configuration to switch.    \u201c ansible-galaxy \u201d automatically creates empty \u201c main.yml \u201d file under \u201c tasks \u201d folder.  We are going to use \u201c Atom \u201d to edit this main.yml file.    On ATOM, open up the project folder  LTRDCN-1572  and edit \u201c main.yml \u201d file under \u201c roles/jinja2_leaf/tasks/ \u201d to include following:   ---\n# tasks file for jinja2_leaf\n  - name: Generate Leaf Config\n    template: src=leaf.j2 dest=roles/jinja2_leaf/files/{{inventory_hostname}}.cfg\n  - name: Push Leaf Config\n    ios_config:\n      src: roles/jinja2_leaf/files/{{inventory_hostname}}.cfg\n      force: yes\n      provider: \"{{ nxos_provider }}\"  Below screenshot shows how the contents of  jinja2_leaf/taks/main.yml  file looks like in Atom:",
            "title": "Step 11: Create playbook for jinja2_leaf role"
        },
        {
            "location": "/task3-vxlan-jinja2/#step-12-run-jinja2_fabric-playbook",
            "text": "In this section you will run the playbook created in step 8, this will generate configuration file for Spine-2 and Leaf-4 switches. It will also push the configuration file to both switches.     Before running the ansible-playbook, you may  log  into the leaf-4 (in MTputty SSH session) and verify that no bgp configurations exist by running  show running bgp  command as shown below:     NOTE: It might take couple of minutes for the configuration to be pushed to via the Ansible Server. It is working in the background.     On the Ansible node (in MTputty SSH session), run the command ( ansible-playbook jinja2_fabric.yml ) to execute the playbook as shown below:  [root@rhel7-tools LTRDCN-1572]# ansible-playbook jinja2_fabric.yml  Below screenshot shows the output of above command:     After the configuration push is successful,  login  (on MTputty SSH session) to  leaf-4  switch to verify configuration has been pushed by running below command:  show running-config bgp  the output of above command is shown below:      Congrats: you have successfully concluded this task by using jinja2 templates with Ansible for Cisco Nexus switches",
            "title": "Step 12: Run Jinja2_fabric playbook"
        }
    ]
}